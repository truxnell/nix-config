{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"readme.md","text":"<p>\ud83d\udc4b Welcome to my NixoOS home and homelab configuration. This monorepo is my personal  nix/nixos setup for all my devices, specifically my homelab.</p> <p>This is the end result of a recovering  k8s addict - who no longer enjoyed the time and effort I personally found it took to run k8s at home.</p>"},{"location":"#why","title":"Why?","text":"<p>Having needed a break from hobby's for some health related reasons, I found coming back to a unpatched cluster a chore, which was left unattented. Then a cheap SSD in my custom VyOS router blew, leading me to just put back in my Unifi Dreammachine router, which broke the custom DNS I was running for my cluster, which caused it issues.</p> <p>During fixing the DNS issue, a basic software upgrade for the custom k8s OS I was running k8s on broke my cluster for the 6th time running, coupled with using a older version of the script tool I used to manage its machine config yaml, which ended up leading to my 6th k8s disaster recovery ).</p> <p>Looking at my boring  Ubuntu ZFS nas which just ran and ran and ran without needing TLC, and remembering the old days with Ubuntu + Docker Compose being hands-off ), I dove into nix, with the idea of getting back to basics of boring proven tools, with the power of nix's declarative system.</p>"},{"location":"#goals","title":"Goals","text":"<p>One of my goals is to bring what I learnt running k8s at home with some of the best homelabbers, into the nix world and see just how much of the practices I learnt I can apply to a nix setup, while focussing on having a solid, reliable, setup that I can leave largely unattended for months without issues cropping up.</p> <p>The goal of this doc is for me to slow down a bit and jot down how and why I am doing what im doing in a module, and cover how I have approached the faucets of homelabbing, so YOU can understand, steal with pride from my code, and hopefully(?) learn a thing or two.</p> <p>To teach me a thing or two, contact me or raise a Issue. PR's may or may not be taken as a personal attack - this is my home setup after all.</p>"},{"location":"motd/","title":"Message of the day","text":"<p>Why not include a nice message of the day for each server I log into?</p> <p>The below gives some insight into what the servers running, status of zpools, usage, etc. While not show below - thankfully - If a zpool error is found the status gives a full <code>zpool status -x</code> debrief which is particulary eye-catching upon login.</p> <p></p> <p>Code TLDR</p> <p>/nixos/modules/nixos/system/motd</p> <p>Write a shell script using nix with a bash motd</p> <pre><code>let\n  motd = pkgs.writeShellScriptBin \"motd\"\n    ''\n      #! /usr/bin/env bash\n      source /etc/os-release\n      service_status=$(systemctl list-units | grep podman-)\n\n      &lt;- SNIP -&gt;\n      printf \"$BOLDService status$ENDCOLOR\\n\"\n    '';\nin\n</code></pre> <p>This gets us a shells script we can then directly call into systemPackages - and after that its just a short hop to make this part of the shell init.</p> <p>Note</p> <p>Replace with your preferred shell!</p> <pre><code>environment.systemPackages = [\n    motd\n];\nprograms.fish.interactiveShellInit =  ''\n    motd\n'';\n</code></pre>"},{"location":"tips/","title":"Tips","text":"<ul> <li>Dont make conditional imports (nix needs to resolve imports upfront)</li> <li>can pass between nixos and home-manager with config.homemanager.users.. and osConfig.&lt;x? <li>when adding home-manager to existing setup, the home-manager service may fail due to trying to over-write existing files in <code>~</code>.  Deleting these should allow the service to start</li> <li>yaml = json, so using nix + builtins.toJSON a lot (and repl to vscode for testing)</li> <p>checking values:</p>"},{"location":"tips/#httpsgithubcomnixosnixpkgsblob90055d5e616bd943795d38808c94dbf0dd35abe8nixosmodulesconfigusers-groupsnixl116","title":"https://github.com/NixOS/nixpkgs/blob/90055d5e616bd943795d38808c94dbf0dd35abe8/nixos/modules/config/users-groups.nix#L116","text":""},{"location":"maintenance/backups/","title":"Backups","text":"<p>Nightly Backups are facilitated by NixOS's module for restic module and a helper module ive written.</p> <p>This does a nightly ZFS snapshot, in which apps and other mutable data is restic backed up to both a local folder on my NAS and also to Cloudflare R2 ). Backing up from a ZFS snapshot ensures that the restic backup is consistent, as backing up files in use (especially a sqlite database) will cause corruption. Here, all restic jobs are backing up as per the 2.05 snapshot, regardless of when they run that night.</p> <p>Another benefit of this approach is that it is service agnostic - containers, nixos services, qemu, whatever all have files in the same place on the filesystem (in the persistant folder) so they can all be backed up in the same fashion.</p> <p>The alternative is to shutdown services during backup (which could be facilitaed with the restic backup pre/post scripts) but ZFS snapshots are a godsend in this area, and im already running them for impermanence.</p> <p>Backing up without snapshots/shutdowns?</p> <p>This is a pattern I see a bit too - if you are backing up files raw without stopping your service beforehand you might want to check to ensure your backups aren't corrupted.</p> <p>The timeline then is:</p> time activity 02.00 ZFS deletes prior snapshot and creates new one, to <code>rpool/safe/persist@restic_nightly_snap</code> 02.05 - 04.05 Restic backs up from new snapshot's hidden read-only mount <code>.zfs</code> with random delays per-service - to local and remote locations"},{"location":"maintenance/backups/#automatic-backups","title":"Automatic Backups","text":"<p>I have added a sops secret for both my local and remote servers in my restic module  /nixos/modules/nixos/services/restic/. This provides the restic password and 'AWS' credentials for the S3-compatible R2 bucket.</p> <p>Backups are created per-service in each services module. This is largely done with a <code>lib</code> helper ive written, which creates both the relevant restic backup local and remote entries in my nixosConfiguration.  nixos/modules/nixos/lib.nix</p> <p>Why not backup the entire persist in one hit?</p> <p>Possibly a hold over from my k8s days, but its incredibly useful to be able to restore per-service, especially if you just want to move an app around or restore one app.  You can always restore multiple repos with a script/taskfile.</p> <p>NixOS will create a service + timer for each job - below shows the output for a prowlarr local/remote backup.</p> <pre><code>truxnell@daedalus ~&gt; systemctl list-unit-files | grep restic-backups-prowlarr\nrestic-backups-prowlarr-local.service                                         linked          enabled\nrestic-backups-prowlarr-remote.service                                        linked          enabled\nrestic-backups-prowlarr-local.timer                                           enabled         enabled\nrestic-backups-prowlarr-remote.timer                                          enabled         enabled\n</code></pre> <p>NixOS (as of 23.05 IIRC) now provides shims to enable easy access to the restic commands with the correct env vars mounted same as the service.</p> <pre><code>truxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n2 snapshots\n</code></pre>"},{"location":"maintenance/backups/#manually-backing-up","title":"Manually backing up","text":"<p>They are a systemd timer/service so you can query or trigger a manual run with <code>systemctl start restic-backups-&lt;service&gt;-&lt;destination&gt;</code> Local and remote work and function exactly the same, querying remote it just a fraction slower to return information.</p> <pre><code>truxnell@daedalus ~ &gt; sudo systemctl start restic-backups-prowlarr-local.service\n&lt; no output &gt;\ntruxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n2 snapshots\ntruxnell@daedalus ~&gt; date\nTue Apr 16 12:43:20 AEST 2024\ntruxnell@daedalus ~&gt;\n</code></pre>"},{"location":"maintenance/backups/#restoring-a-backup","title":"Restoring a backup","text":"<p>Testing a restore (would do --target / for a real restore) Would just have to pause service, run restore, then re-start service.</p> <pre><code>truxnell@daedalus ~ [1]&gt; sudo restic-lidarr-local restore --target /tmp/lidarr/ latest\nrepository a2847581 opened (version 2, compression level auto)\n[0:00] 100.00%  2 / 2 index files loaded\nrestoring &lt;Snapshot b96f4b94 of [/persist/nixos/lidarr] at 2024-04-14 04:19:41.533770692 +1000 AEST by root@daedalus&gt; to /tmp/lidarr/\nSummary: Restored 52581 files/dirs (11.025 GiB) in 1:37\n</code></pre>"},{"location":"maintenance/backups/#failed-backup-notifications","title":"Failed backup notifications","text":"<p>Failed backup notifications are baked-in due to the global Pushover notification on SystemD unit falure. No config nessecary</p> <p>Here I tested it by giving the systemd unit file a incorrect path.</p> A deliberately failed backup to test notifications, hopefully I don't see a real one."},{"location":"maintenance/backups/#disabled-backup-warnings","title":"Disabled backup warnings","text":"<p>Using module warnings, I have also put in warnings into my NixOS modules if I have disabled a warning on a host that isnt a development machine, just in case I do this or mixup flags on hosts. Roll your eyes, I will probably do it. This will pop up when I do a dry run/deployment - but not abort the build.</p> It is eye catching thankfully"},{"location":"maintenance/software_updates/","title":"Software updates","text":"<p>Its crucial to update software regularly - but a homelab isn't a google play store you forget about and let it do its thing. How do you update your software stack regular without breaking things?</p>"},{"location":"maintenance/software_updates/#continuous-integration","title":"Continuous integration","text":"<p>Continuous integration (CI) is running using  Github Actions and Garnix. I have enabled branch protection rules to ensure all my devices successfully build before a PR is allowed to be pushed to main. This ensures I have a level of testing/confidence that an update of a device from the main branch will not break anything.</p> Lovely sea of green passed checks"},{"location":"maintenance/software_updates/#binary-caching","title":"Binary Caching","text":"<p>Binary caching is done for me by Garnix which is an amazing tool. I can then add them as substituter. These run each push to any branch and cache the build results for me. Even better, I can hook into them as above for CI purposes. No code to show here, you add it as an app to your github repo and it 'Just Works '</p> <pre><code># Substitutions\nsubstituters = [ \"https://cache.garnix.io\" ];\n\ntrusted-public-keys = [\n  \"nix-community.cachix.org-1:mB9FSh9qf2dCimDSUo8Zy7bkq5CX+/rkCWyvRCYg3Fs=\"\n];\n</code></pre> Lovely sea of green passed checks"},{"location":"maintenance/software_updates/#flake-updates","title":"Flake updates","text":"<p>Github repo updates are provided by  Renovate by Mend. These are auto-merged on a weekly schedule after passing CI. The settings can be found at  /main/.github/renovate.json5</p> <p>The primary CI is a Garnix build, which Is already building and caching all my systems. Knowing all of the systems have built and cached goes a huge way toward ensuring main is a stable branch.</p>"},{"location":"maintenance/software_updates/#docker-container-updates","title":"Docker container updates","text":"<p>Container updates are provided by  Renovate by Mend. These will either be manually merged after I have checked the upstream projects notes for breaking changes or auto-merged based on settings I have in  /.github/renovate/autoMerge.json5.</p> <p>Semantic Versioning summary</p> <p>Semantic Versioning blurb is a format of MAJOR.MINOR.PATCH: MAJOR version when you make incompatible API changes (e.g. 1.7.8 -&gt; 2.0.0) MINOR version when you add functionality in a backward compatible manner (e.g. 1.7.8 -&gt; 1.8.0) PATCH version when you make backward compatible bug fixes (e.g. 1.7.8 -&gt; 1.7.9)</p> <p>The auto-merge file allows me to define a pattern of which packages I want to auto-merge based on the upgrade type Renovate is suggesting. As many packages adhere to Semantic Versioning, I can determine how I 'feel' about the project, and decide to auto-merge specific tags. So for example, Sonarr has been reliable for me so I am ok merging all digest, patch and minor updates. I will always review a a major update, as it is likely to contain a breaking change.</p> <p>Respect pre-1.0.0 software!</p> <p>Semantic Versioning also specifies that all software before 1.0.0 may have a breaking change AT ANY TIME. Auto update pre 1.0 software at your own risk!</p> <p>The rational here is twofold. One is obvious - The entire point of doing Nix is reproducibility - what is the point of having flakes and SHA tags to provide the ability</p> <p>Also, I dont wan't a trillion PR's in my github repo waiting, but I also will not blindly update everything. There is a balance between updating for security/patching purposes and avoiding breaking changes. I know its popular to use <code>:latest</code> tag and a auto-update service like watchtower - trust me this is a bad idea.</p> I only glanced away from my old homelab for a few months... <p>Automatically updating all versions of containers will break things eventually!</p> <p>This is simply because projects from time to time will release breaking changes - totally different database schemas, overhaul config, replace entire parts of their software stack etc.  If you let your service update totally automatically without checking for these you will wake up to a completely broken service like I did many, many years ago when Seafile did a major upgrade.</p> <p>Container updates are provided by a custom regex that matches my format for defining images in my nix modules.</p> <pre><code>    \"regexManagers\": [\n    {\n      fileMatch: [\"^.*\\\\.nix$\"],\n      matchStrings: [\n        'image *= *\"(?&lt;depName&gt;.*?):(?&lt;currentValue&gt;.*?)(@(?&lt;currentDigest&gt;sha256:[a-f0-9]+))?\";',\n      ],\n      datasourceTemplate: \"docker\",\n    }\n  ],\n</code></pre> <p>And then I can pick and choose what level (if any) I want for container software. The below gives me brackets I can put containers in to enable auto-merge depending on how much I much i trust the container maintainer.</p> <pre><code>  \"packageRules\": [\n    {\n      // auto update up to major\n      \"matchDatasources\": ['docker'],\n      \"automerge\": \"true\",\n      \"automergeType\": \"branch\",\n      \"matchUpdateTypes\": [ 'minor', 'patch', 'digest'],\n      \"matchPackageNames\": [\n        'ghcr.io/onedr0p/sonarr',\n        'ghcr.io/onedr0p/readarr',\n        'ghcr.io/onedr0p/radarr',\n        'ghcr.io/onedr0p/lidarr',\n        'ghcr.io/onedr0p/prowlarr'\n        'ghcr.io/twin/gatus',\n      ]\n    },\n    // auto update up to minor\n    {\n      \"matchDatasources\": ['docker'],\n      \"automerge\": \"true\",\n      \"automergeType\": \"branch\",\n      \"matchUpdateTypes\": [ 'patch', 'digest'],\n      \"matchPackageNames\": [\n        \"ghcr.io/gethomepage/homepage\",\n      ]\n\n    }\n  ]\n</code></pre> <p>Which results in automated PR's being raised - and possibly automatically merged into main if CI passes.</p> Thankyou RenovateBot!"},{"location":"monitoring/systemd/","title":"SystemD pushover notifications","text":"<p>Keeping with the goal of simple, I put together a <code>curl</code> script that can send me a pushover alert. I originally tied this to individual backups, until I realised how powerful it would be to just have it tied to every SystemD service globally.</p> <p>This way, I would never need to worry or consider what services are being created/destroyed and repeating myself ad nauseam.</p> <p>Why not Prometheus?</p> <p>I ran Prometheus/AlertManager for many years and well it can be easy to get TOO many notifications depending on your alerts, or to have issues with the big complex beast it is itself, or have alerts that trigger/reset/trigger (i.e. HDD temps). This gives me native, simple notifications I can rely on using basic tools - one of my design principles.</p> <p>Immediately I picked up with little effort:</p> <ul> <li>Pod crashloop failed after too many quick restarts</li> <li>Native service failure</li> <li>Backup failures</li> <li>AutoUpdate failure</li> <li>etc</li> </ul> NixOS SystemD built-in notifications for all occasions"},{"location":"monitoring/systemd/#adding-to-all-services","title":"Adding to all services","text":"<p>This is accomplished in /nixos/modules/nixos/system/pushover, with a systemd service <code>notify-pushover@</code>.</p> <p>This can then be called by other services, which I setup with adding into my options:</p> <pre><code>  options.systemd.services = mkOption {\n    type = with types; attrsOf (\n      submodule {\n        config.onFailure = [ \"notify-pushover@%n.service\" ];\n      }\n    );\n</code></pre> <p>This adds into every systemd NixOS generates the \"notify-pushover@%n.service\", where the systemd specifiers are injected with <code>scriptArgs</code>, and the simple bash script can refer to them as <code>$1</code> etc.</p> <pre><code>systemd.services.\"notify-pushover@\" = {\n      enable = true;\n      onFailure = lib.mkForce [ ]; # cant refer to itself on failure (1)\n      description = \"Notify on failed unit %i\";\n      serviceConfig = {\n        Type = \"oneshot\";\n        # User = config.users.users.truxnell.name;\n        EnvironmentFile = config.sops.secrets.\"services/pushover/env\".path; # (2)\n      };\n\n      # Script calls pushover with some deets.\n      # Here im using the systemd specifier %i passed into the script,\n      # which I can reference with bash $1.\n      scriptArgs = \"%i %H\"; # (3)\n      # (4)\n      script = ''\n        ${pkgs.curl}/bin/curl --fail -s -o /dev/null \\\n          --form-string \"token=$PUSHOVER_API_KEY\" \\\n          --form-string \"user=$PUSHOVER_USER_KEY\" \\\n          --form-string \"priority=1\" \\\n          --form-string \"html=1\" \\\n          --form-string \"timestamp=$(date +%s)\" \\\n          --form-string \"url=https://$2:9090/system/services#/$1\" \\\n          --form-string \"url_title=View in Cockpit\" \\\n          --form-string \"title=Unit failure: '$1' on $2\" \\\n          --form-string \"message=&lt;b&gt;$1&lt;/b&gt; has failed on &lt;b&gt;$2&lt;/b&gt;&lt;br&gt;&lt;u&gt;Journal tail:&lt;/u&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;$(journalctl -u $1 -n 10 -o cat)&lt;/i&gt;\" \\\n          https://api.pushover.net/1/messages.json 2&amp;&gt;1\n      '';\n</code></pre> <ol> <li>Force exclude this service from having the default 'onFailure' added</li> <li>Bring in pushover API/User ENV vars for script</li> <li>Pass SystemD specifiers into script</li> <li>Er.. script. Nix pops it into a shell script and refers to it in the unit.</li> </ol> <p>Bug</p> <p>I put in a nice link direct to Cockpit for the specific machine/service in question that doesnt quite work yet... ( #96)</p>"},{"location":"monitoring/systemd/#excluding-from-a-services","title":"Excluding from a services","text":"<p>Now we may not want this on ALL services. Especially the pushover-notify service itself. We can exclude this from a service using Nix <code>nixpkgs.lib.mkForce</code></p> <pre><code># Over-write the default pushover\nsystemd.services.\"service\".onFailure = lib.mkForce [ ] option.\n</code></pre>"},{"location":"monitoring/warnings/","title":"Nix Warnings","text":"<p>I've added warnings and assertations to code using nix to help me avoid misconfigurations. For example, if a module needs a database enabled, it can abort a deployment if it is not enabled. Similary, I have added warnings if I have disabled backups for production machines.</p> <p>But why, when its not being shared with others?</p> <p>Because I guarentee ill somehow stuff it up down the track and accidently disable things I didnt mean to. Roll your eyes, Ill thank myself later.</p> <p>Learnt from: Nix Manual</p>"},{"location":"monitoring/warnings/#warnings","title":"Warnings","text":"<p>Warnings will print a warning message duyring a nix build or deployment, but NOT stop the action. Great for things like reminders on disabled features</p> <p>To add a warning inside a module:</p> <pre><code>    # Warn if backups are disable and machine isn't a dev box\n    config.warnings = [\n      (mkIf (!cfg.local.enable &amp;&amp; config.mySystem.purpose != \"Development\")\n        \"WARNING: Local backups are disabled!\")\n      (mkIf (!cfg.remote.enable &amp;&amp; config.mySystem.purpose != \"Development\")\n        \"WARNING: Remote backups are disabled!\")\n    ];\n</code></pre> Oh THATS what I forgot to re-enable..."},{"location":"monitoring/warnings/#abortassert","title":"Abort/assert","text":"<p>Warnings bigger and meaner brother. Stops a nix build/deploy dead in its tracks. Only useful for when deployment is incompatiable with running - i.e. a dependency not met in options.</p>"},{"location":"overview/design/","title":"Design principles","text":"<p>Taking some lead from the Zen of Python:</p> <ul> <li>Minimise dependencies, where required, explicitly define dependencies</li> <li>Use plain Nix &amp; bash to solve problems over additional tooling</li> <li>Stable channel for stable machines. Unstable only where features are important.</li> <li>Modules for a specific service - Profiles for broad configuration of state.</li> <li>Write readable code - descriptive variable names and modules</li> <li>Keep functions/dependencies within the relevant module where possible</li> <li>Errors should never pass silently - use assert etc for misconfigurations</li> <li>Flat is better than nested - use built-in functions like map, filter, and fold to operate on lists or sets</li> </ul>"},{"location":"overview/features/","title":"Features","text":"<p>Some things I'm proud of. Or just happy they exist so I can forget about something until I need to worry.</p> <ul> <li> Nightly BackupsA ZFS snapshot is done at night, with restic then backing up to both locally and cloud.  NixOS wrappers make restoring a single command line entry.ZFS snapshot before backup is important to ensure restic isnt backing up files that are in use, which would cause corruption.</li> <li> Software UpdatesRenovate Bot regulary runs on this Github repo, updating the flake lockfile, containers and other dependencies automatically. Automerge is enabled for updates I expect will be routine, but waits for manual PR approval for updates I suspect may require reading changelog for breaking changes</li> <li> Impermance:Inspried by the Erase your Darlings post, Servers run zfs and rollback to a blank snapshot at night.  This ensures repeatable NixOS deployments and no cruft, and also hardens servers a little.</li> <li> SystemD Notifications:Systemd hook that adds a pushover notification to any systemd unit failure for any unit NixOS is aware of.  No worrying about forgetting to add a notification to every new service or worrying about missing one.</li> </ul>"},{"location":"overview/goals/","title":"Goals","text":"<p>When I set about making this lab I had a number of goals - I wonder how well I will do ?</p> <p>A master list of ideas/goals/etc can be found at  Issue #1</p> <ul> <li> Stability NixOS stable channel for core services unstable for desktop apps/non-mission critical where desired. Containers with SHA256 pinning for server apps</li> <li> KISSKeep it Simple, use boring, reliable, trusted tools - not todays flashy new software repo</li> <li> Easy UpdatesWeekly update schedule, utilizing Renovate for updating lockfile and container images.  Autoupdates enabled off main branch for mission critical. Aim for 'magic rollback' on upgrade failure</li> <li> BackupsNightly restic backups to both cloud and NAS. All databases to have nightly backups. Test backups regulary</li> <li> ReproducabilityFlakes &amp; Git for version pinning, SHA256 tags for containers.</li> <li> MonitoringAutomated monitoring on failure &amp; critical summaries, using basic tools. Use Gatus for both internal and external monitoring</li> <li> Continuous IntegrationCI against main branch to ensure all code compiles OK. Use PR's to add to main and dont skip CI due to impatience</li> <li> SecurityDont use containers with S6 overlay/root (i.e. LSIO ). Expose minimal ports at router, Reduce attack surface by keeping it simple, review hardening containers/podman/NixOS</li> <li> Ease of administrationLean into the devil that is SystemD - and have one standard interface to see logs, manipulate services, etc. Run containers as podman services, and webui's for watching/debugging</li> <li> Secrets <sub>ssshh</sub>..Sops-nix for secrets, living in my gitrepo. Avoid cloud services like I used in k8s (i.e. Doppler.io)</li> </ul>"},{"location":"overview/options/","title":"Options","text":"<p>Explain mySystem and myHome</p>"},{"location":"overview/structure/","title":"Repository Structure","text":"<p>Note</p> <p>Oh god writing this now is a horrid idea, I always refactor like 50 times...</p> <p>Here is a bit of a walkthrough of the repository structure so you I can have a vague idea on what is going on. Organizing a monorepo is hard at the best of times. </p> <pre><code>\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 renovate            Renovate modules\n\u2502   \u251c\u2500\u2500 workflows             Github Action workflows (i.e. CI/Site building)\n\u2502   \u2514\u2500\u2500 renovate.json5        Renovate core settings\n\u251c\u2500\u2500 .taskfiles              go-task file modules\n\u251c\u2500\u2500 docs                    This mkdocs-material site\n\u2502   nixos                   Nixos Modules\n\u2502   \u2514\u2500\u2500 home                  home-manager nix files\n\u2502       \u251c\u2500\u2500 modules             home-manager modules\n\u2502       \u2514\u2500\u2500 truxnell            home-manager user\n\u2502   \u251c\u2500\u2500 hosts                 hosts for nix - starting point of configs.\n\u2502   \u251c\u2500\u2500 modules               nix modules\n\u2502   \u251c\u2500\u2500 overlays              nixpkgs overlays\n\u2502   \u251c\u2500\u2500 pkgs                  custom nix packages\n\u2502   \u2514\u2500\u2500 profiles              host profiles\n\u251c\u2500\u2500 README.md               Github Repo landing page\n\u251c\u2500\u2500 flake.nix               Core flake\n\u251c\u2500\u2500 flake.lock              Lockfile\n\u251c\u2500\u2500 LICENSE                 Project License\n\u251c\u2500\u2500 mkdocs.yml              mkdocs settings\n\u2514\u2500\u2500 Taskfile.yaml           go-task core file\n</code></pre> <p>Whew that wasnt so hard right... right?</p>"},{"location":"security/containers/","title":"Containers","text":""},{"location":"security/containers/#container-images","title":"Container images","text":"<p>Dont use LSIO!</p>"},{"location":"vm/faq/","title":"Faq","text":""},{"location":"vm/faq/#why-not-recurse-the-module-folder","title":"Why not recurse the module folder","text":"<p>Imports are special in NIX and its important that they are definet at runtime for lazy evaluation - if you do optional/coded imports not everything is avaliable for evaluating.</p>"},{"location":"vm/impermance/","title":"Impermance","text":"<ul> <li>need to save ssh keys on reboot</li> <li>else you end up with sops issues &amp; ssh known_key changes every reboot</li> <li>need to sort out password</li> </ul>"},{"location":"vm/installing-x86_64/","title":"Installing x86 64","text":""},{"location":"vm/installing-x86_64/#installing-a-playground-vm","title":"Installing a playground VM","text":"<p>I've used gnome-boxes from my current Fedora laptop for running playground vm's.</p> <p>Settings: ISO: nixos-minimal Hard drive: 32GB RAM: 2GB EFI: Enable</p> <p>Expose port 22 to allow ssh into vm (host port 3022, guest 22)</p> <pre><code># set temp root passwd\nsudo su\npasswd\n</code></pre> <p><code>sshd</code> is already running, so you can now ssh into the vm remotely for the rest of the setup. <code>ssh root@127.0.0.1 -p 3022</code></p> <pre><code># Partitioning\nparted /dev/sda -- mklabel gpt\nparted /dev/sda -- mkpart root ext4 512MB -8GB\nparted /dev/sda -- mkpart swap linux-swap -8GB 100%\nparted /dev/sda -- mkpart ESP fat32 1MB 512MB\nparted /dev/sda -- set 3 esp on\n\n# Formatting\nmkfs.ext4 -L nixos /dev/sda1\nmkswap -L swap /dev/sda2\nmkfs.fat -F 32 -n boot /dev/sda3\n\n# Mounting disks for installation\nmount /dev/disk/by-label/nixos /mnt\nmkdir -p /mnt/boot\nmount /dev/disk/by-label/boot /mnt/boot\nswapon /dev/sda2\n\n# Generating default configuration\nnixos-generate-config --root /mnt\n</code></pre> <p>From this config copy the bootstrap configuration and fetch the hardware configuration.</p> <pre><code>scp -P 3022 nixos/hosts/bootstrap/configuration.nix root@127.0.0.1:/mnt/etc/nixos/configuration.nix\nscp -P 3022 root@127.0.0.1:/mnt/etc/nixos/hardware-configuration.nix nixos/hosts/nixosvm/hardware-configuration.nix\n</code></pre> <p>Then back to the VM</p> <pre><code>nixos-install\nreboot\nnixos-rebuild switch\n</code></pre> <p>Set the password for the user that was created. Might need to use su?</p> <pre><code>passwd truxnell\n</code></pre> <p>Also grab the ssh keys and re-encrypt sops</p> <pre><code>cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age\n</code></pre> <p>then run task</p> <p>Login as user, copy nix git OR for remote machines/servers just <code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;</code></p> <pre><code>mkdir .local\ncd .local\ngit clone https://github.com/truxnell/nix-config.git\ncd nix-config\n</code></pre> <p>Apply config to bootstrapped device First time around, MUST APPLY  with name of host in ./hosts/ This is because <code>.. --flake .</code> looks for a <code>nixosConfigurations</code> key with the machines hostname The bootstrap machine will be called 'nixos-bootstrap' so the flake by default would resolve <code>nixosConfigurations.nixos-bootstrap</code> Subsequent rebuilds can be called with the default command as after first build the machines hostname will be changed to the desired machine <pre><code>nixos-rebuild switch --flake .#&lt;machinename&gt;\n</code></pre>"},{"location":"vm/installing-zfs-impermance/","title":"Installing zfs impermance","text":"<p>https://grahamc.com/blog/erase-your-darlings/</p>"},{"location":"vm/installing-zfs-impermance/#get-hostid","title":"Get hostid","text":"<p>run <code>head -c 8 /etc/machine-id</code> and copy into networking.hostId to ensure ZFS doesnt get borked on reboot</p>"},{"location":"vm/installing-zfs-impermance/#partitioning","title":"Partitioning","text":"<p>parted /dev/nvme0n1 -- mklabel gpt parted /dev/nvme0n1 -- mkpart root ext4 512MB -8GB parted /dev/nvme0n1 -- mkpart swap linux-swap -8GB 100% parted /dev/nvme0n1 -- mkpart ESP fat32 1MB 512MB parted /dev/nvme0n1 -- set 3 esp on</p>"},{"location":"vm/installing-zfs-impermance/#formatting","title":"Formatting","text":"<p>mkswap -L swap /dev/nvme0n1p2 swapon /dev/nvme0n1p2 mkfs.fat -F 32 -n boot /dev/nvme0n1p3</p>"},{"location":"vm/installing-zfs-impermance/#zfs-on-root-partition","title":"ZFS on root partition","text":"<p>zpool create -O mountpoint=none rpool /dev/nvme0n1p1</p> <p>zfs create -p -o mountpoint=legacy rpool/local/root</p>"},{"location":"vm/installing-zfs-impermance/#immediate-blank-snapshot","title":"immediate blank snapshot","text":"<p>zfs snapshot rpool/local/root@blank mount -t zfs rpool/local/root /mnt</p>"},{"location":"vm/installing-zfs-impermance/#boot-partition","title":"Boot partition","text":"<p>mkdir /mnt/boot mount /dev/nvme0n1p3 /mnt/boot</p>"},{"location":"vm/installing-zfs-impermance/#mk-nix","title":"mk nix","text":"<p>zfs create -p -o mountpoint=legacy rpool/local/nix mkdir /mnt/nix mount -t zfs rpool/local/nix /mnt/nix</p>"},{"location":"vm/installing-zfs-impermance/#and-a-dataset-for-home-if-needed","title":"And a dataset for /home: if needed","text":"<p>zfs create -p -o mountpoint=legacy rpool/safe/home mkdir /mnt/home mount -t zfs rpool/safe/home /mnt/home</p> <p>zfs create -p -o mountpoint=legacy rpool/safe/persist mkdir /mnt/persist mount -t zfs rpool/safe/persist /mnt/persist</p> <p>Set <code>networking.hostid`` in the nixos config to</code>head -c 8 /etc/machine-id`</p> <pre><code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;\n</code></pre> <p>consider a nixos-enter to import a zpool if required (for NAS) instead of rebooting post-install</p>"},{"location":"vm/k8s/","title":"K8s","text":"<p>Removed complexity</p> <ul> <li>external secrets -&gt; bog standard sops</li> <li>HA file storage -&gt; standard file system</li> <li>HA database cluster -&gt; nixos standard cluster</li> <li>Database user operator -&gt; nixos standard ensure_users</li> <li>Database permissions operator -&gt; why even??</li> <li>secrets reloader -&gt; sops restart_unit</li> <li>easier managment, all services run through systemd for consistency, cockpit makes viewing logs/pod console etc easy.</li> </ul>"},{"location":"vm/secrets/","title":"Generate age key per machine","text":"<p>On new machine, run below to transfer its shiny new ed25519 to age</p> <pre><code>nix-shell -p ssh-to-age --run 'cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age'\n</code></pre> <p>Copy this into <code>./.sops.yaml</code> in base repo, then re-run taskfile <code>task sops:re-encrypt</code> to loop through all sops keys, decrypt then re-encrypt</p>"}]}