{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"readme.md","text":"<p>\ud83d\udc4b Welcome to my NixoOS home and homelab configuration. This monorepo is my personal  nix/nixos setup for all my devices, specifically my homelab.</p> <p>This is the end result of a recovering  k8s addict - who no longer enjoyed the time and effort I personally found it took to run k8s at home.</p>"},{"location":"#why","title":"Why?","text":"<p>Having needed a break from hobby's for some health related reasons, I found coming back to a unpatched cluster a chore, which was left unattented. Then a cheap SSD in my custom VyOS router blew, leading me to just put back in my Unifi Dreammachine router, which broke the custom DNS I was running for my cluster, which caused it issues.</p> <p>During fixing the DNS issue, a basic software upgrade for the custom k8s OS I was running k8s on broke my cluster for the 6th time running, coupled with using a older version of the script tool I used to manage its machine config yaml, which ended up leading to my 6th k8s disaster recovery ).</p> <p>Looking at my boring  Ubuntu ZFS nas which just ran and ran and ran without needing TLC, and remembering the old days with Ubuntu + Docker Compose being hands-off ), I dove into nix, with the idea of getting back to basics of boring proven tools, with the power of nix's declarative system.</p>"},{"location":"#goals","title":"Goals","text":"<p>One of my goals is to bring what I learnt running k8s at home with some of the best homelabbers, into the nix world and see just how much of the practices I learnt I can apply to a nix setup, while focussing on having a solid, reliable, setup that I can leave largely unattended for months without issues cropping up.</p> <p>The goal of this doc is for me to slow down a bit and jot down how and why I am doing what im doing in a module, and cover how I have approached the faucets of homelabbing, so YOU can understand, steal with pride from my code, and hopefully(?) learn a thing or two.</p> <p>To teach me a thing or two, contact me or raise a Issue. PR's may or may not be taken as a personal attack - this is my home setup after all.</p>"},{"location":"motd/","title":"Message of the day","text":"<p>Why not include a nice message of the day for each server I log into?</p> <p>The below gives some insight into what the servers running, status of zpools, usage, etc. While not show below - thankfully - If a zpool error is found the status gives a full <code>zpool status -x</code> debrief which is particulary eye-catching upon login.</p> <p>I've also squeezed in a 'reboot required' flag for when the server had detected its running kernel/init/systemd is a different version to what it booted with - useful to know when long running servers require a reboot to pick up new kernel/etc versions.</p> Message of the day <p>Code TLDR</p> <p>/nixos/modules/nixos/system/motd</p> <p>Write a shell script using nix with a bash motd of your choosing.</p> <pre><code>let\n  motd = pkgs.writeShellScriptBin \"motd\"\n    ''\n      #! /usr/bin/env bash\n      source /etc/os-release\n      service_status=$(systemctl list-units | grep podman-)\n\n      &lt;- SNIP -&gt;\n      printf \"$BOLDService status$ENDCOLOR\\n\"\n    '';\nin\n</code></pre> <p>This gets us a shells script we can then directly call into systemPackages - and after that its just a short hop to make this part of the shell init.</p> <p>Note</p> <p>Replace with your preferred shell!</p> <pre><code>environment.systemPackages = [\n    motd\n];\nprograms.fish.interactiveShellInit =  ''\n    motd\n'';\n</code></pre>"},{"location":"postgres/","title":"Postgres","text":"<p>to restore: dropdb createdb alter owner of db psql &lt; restore.sql</p>"},{"location":"tips/","title":"Tips","text":"<ul> <li>Dont make conditional imports (nix needs to resolve imports upfront)</li> <li>can pass between nixos and home-manager with config.homemanager.users.. and osConfig.&lt;x? <li>when adding home-manager to existing setup, the home-manager service may fail due to trying to over-write existing files in <code>~</code>.  Deleting these should allow the service to start</li> <li>yaml = json, so using nix + builtins.toJSON a lot (and repl to vscode for testing)</li> <p>checking values:</p>"},{"location":"tips/#httpsgithubcomnixosnixpkgsblob90055d5e616bd943795d38808c94dbf0dd35abe8nixosmodulesconfigusers-groupsnixl116","title":"https://github.com/NixOS/nixpkgs/blob/90055d5e616bd943795d38808c94dbf0dd35abe8/nixos/modules/config/users-groups.nix#L116","text":""},{"location":"development/code-quality/","title":"Code Quality Standards","text":"<p>This document outlines code quality standards, linting rules, and best practices for this repository.</p>"},{"location":"development/code-quality/#overview","title":"Overview","text":"<p>We maintain high code quality through automated tooling, clear conventions, and best practices. All code should be:</p> <ul> <li>Readable: Clear structure and naming</li> <li>Maintainable: Well-organized and documented</li> <li>Consistent: Follows established patterns</li> <li>Tested: Validated through our testing infrastructure</li> </ul>"},{"location":"development/code-quality/#linting-tools","title":"Linting Tools","text":""},{"location":"development/code-quality/#nix-linting","title":"Nix Linting","text":""},{"location":"development/code-quality/#statix","title":"Statix","text":"<p>Purpose: Find and fix antipatterns in Nix code.</p> <p>Usage: <pre><code># Check for issues\nstatix check .\n\n# Check specific path\nstatix check nixos/\n\n# Auto-fix issues (use with caution)\nstatix fix .\n</code></pre></p> <p>Via Justfile: <pre><code>just lint\n</code></pre></p> <p>Configuration: Uses default statix rules. Consider adding <code>.statix.toml</code> for custom rules if needed.</p>"},{"location":"development/code-quality/#deadnix","title":"Deadnix","text":"<p>Purpose: Find unused Nix code (let bindings, function arguments, etc.).</p> <p>Usage: <pre><code>deadnix .\n</code></pre></p> <p>Integration: Available in devShell, can be added to pre-commit if desired.</p>"},{"location":"development/code-quality/#yaml-linting","title":"YAML Linting","text":"<p>Tool: <code>yamllint</code></p> <p>Configuration: <code>.github/lint/.yamllint.yaml</code></p> <p>Usage: Automatically runs via pre-commit hooks.</p>"},{"location":"development/code-quality/#secret-detection","title":"Secret Detection","text":"<p>Tool: <code>gitleaks</code></p> <p>Purpose: Prevent accidental commits of secrets, API keys, passwords, etc.</p> <p>Usage: Automatically runs via pre-commit hooks.</p> <p>Configuration: Uses default patterns. Consider adding <code>.gitleaksignore</code> for false positives.</p>"},{"location":"development/code-quality/#pre-commit-hooks","title":"Pre-Commit Hooks","text":"<p>Pre-commit hooks automatically run checks before commits.</p>"},{"location":"development/code-quality/#setup","title":"Setup","text":"<pre><code>just pre-commit-init\n</code></pre> <p>This installs hooks and dependencies.</p>"},{"location":"development/code-quality/#available-hooks","title":"Available Hooks","text":"<p>Current hooks configured in <code>.pre-commit-config.yaml</code>:</p> <ol> <li>yamllint: YAML syntax and style</li> <li>trailing-whitespace: Remove trailing whitespace</li> <li>end-of-file-fixer: Ensure files end with newline</li> <li>fix-byte-order-marker: Remove BOM characters</li> <li>mixed-line-ending: Enforce consistent line endings</li> <li>check-added-large-files: Prevent large file commits (&gt;2MB)</li> <li>check-merge-conflict: Detect merge conflict markers</li> <li>check-executables-have-shebangs: Verify executable files have shebangs</li> <li>remove-crlf: Remove Windows line endings</li> <li>remove-tabs: Remove tabs (except Makefiles)</li> <li>gitleaks: Detect secrets</li> <li>sops-encryption: Verify SOPS files are encrypted</li> </ol>"},{"location":"development/code-quality/#running-manually","title":"Running Manually","text":"<pre><code># Run on all files\njust pre-commit-run\n\n# Run on staged files only\npre-commit run\n\n# Run specific hook\npre-commit run yamllint\n</code></pre>"},{"location":"development/code-quality/#updating-hooks","title":"Updating Hooks","text":"<pre><code>just pre-commit-update\n</code></pre>"},{"location":"development/code-quality/#code-style-guidelines","title":"Code Style Guidelines","text":""},{"location":"development/code-quality/#nix-code-style","title":"Nix Code Style","text":""},{"location":"development/code-quality/#module-structure","title":"Module Structure","text":"<p>Each application module should follow this structure:</p> <pre><code>{ config, lib, pkgs, ... }:\n\nlet\n  cfg = config.mySystem.services.&lt;app-name&gt;;\nin\n{\n  options.mySystem.services.&lt;app-name&gt; = {\n    enable = lib.mkEnableOption \"&lt;app description&gt;\";\n    # Additional options...\n  };\n\n  config = lib.mkIf cfg.enable {\n    # Configuration...\n  };\n}\n</code></pre>"},{"location":"development/code-quality/#import-paths","title":"Import Paths","text":"<p>Always use explicit paths with <code>/default.nix</code>:</p> <pre><code># \u2705 Good\n../../applications/media/jellyfin/default.nix\n\n# \u274c Bad (unreliable)\n../../applications/media/jellyfin\n</code></pre>"},{"location":"development/code-quality/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Applications: lowercase with hyphens (<code>browserless-chrome</code>, <code>code-server</code>)</li> <li>Options: camelCase in config (<code>config.mySystem.services.browserlessChrome.enable</code>)</li> <li>Categories: lowercase (<code>media</code>, <code>productivity</code>, <code>infrastructure</code>)</li> </ul>"},{"location":"development/code-quality/#option-definitions","title":"Option Definitions","text":"<pre><code># \u2705 Good - use mkEnableOption for boolean flags\nenable = lib.mkEnableOption \"description\";\n\n# \u2705 Good - type-safe options\nport = lib.mkOption {\n  type = lib.types.port;\n  default = 8080;\n  description = \"Port to listen on\";\n};\n\n# \u274c Bad - accessing config during option definition\nenable = lib.mkOption {\n  default = config.mySystem.enableAll;\n};\n</code></pre>"},{"location":"development/code-quality/#yaml-code-style","title":"YAML Code Style","text":"<ul> <li>Use 2 spaces for indentation</li> <li>No trailing whitespace</li> <li>Consistent quoting (prefer no quotes unless needed)</li> <li>Document complex structures</li> </ul>"},{"location":"development/code-quality/#documentation-style","title":"Documentation Style","text":"<ul> <li>Use clear, concise language</li> <li>Include examples where helpful</li> <li>Keep documentation up-to-date with code changes</li> <li>Use proper Markdown formatting</li> </ul>"},{"location":"development/code-quality/#best-practices","title":"Best Practices","text":""},{"location":"development/code-quality/#module-organization","title":"Module Organization","text":"<ol> <li>One module per application: Each app gets its own directory</li> <li>Consistent structure: Follow established patterns</li> <li>Clear dependencies: Import dependencies explicitly</li> <li>Archive unused: Move unused modules to <code>_archive/</code></li> </ol>"},{"location":"development/code-quality/#error-handling","title":"Error Handling","text":"<ul> <li>Use <code>mkIf</code> for conditional configuration</li> <li>Provide clear error messages</li> <li>Validate inputs where possible</li> </ul>"},{"location":"development/code-quality/#performance","title":"Performance","text":"<ul> <li>Avoid unnecessary evaluations</li> <li>Use lazy evaluation where appropriate</li> <li>Cache expensive computations</li> </ul>"},{"location":"development/code-quality/#security","title":"Security","text":"<ul> <li>Never commit secrets (use SOPS)</li> <li>Validate user inputs</li> <li>Follow principle of least privilege</li> <li>Keep dependencies updated</li> </ul>"},{"location":"development/code-quality/#code-review-guidelines","title":"Code Review Guidelines","text":"<p>When reviewing code:</p> <ol> <li>Functionality: Does it work as intended?</li> <li>Style: Follows coding standards?</li> <li>Tests: Includes appropriate tests?</li> <li>Documentation: Updated documentation?</li> <li>Security: No security concerns?</li> <li>Performance: No obvious performance issues?</li> </ol>"},{"location":"development/code-quality/#continuous-improvement","title":"Continuous Improvement","text":"<p>Code quality is an ongoing process:</p> <ul> <li>Regularly update linting rules</li> <li>Refactor when patterns emerge</li> <li>Document decisions and trade-offs</li> <li>Share knowledge with the team</li> </ul>"},{"location":"development/code-quality/#resources","title":"Resources","text":"<ul> <li>NixOS Manual</li> <li>Nix Pills</li> <li>Statix Documentation</li> <li>Pre-commit Documentation</li> </ul>"},{"location":"development/formatting/","title":"Code Formatting Standards","text":"<p>This document outlines the code formatting standards and tools used in this repository.</p>"},{"location":"development/formatting/#overview","title":"Overview","text":"<p>We maintain consistent code formatting across all files to improve readability and reduce merge conflicts. All formatting is automated through tools configured in the development environment.</p>"},{"location":"development/formatting/#nix-files","title":"Nix Files","text":""},{"location":"development/formatting/#tool-nixpkgs-fmt","title":"Tool: <code>nixpkgs-fmt</code>","text":"<p>All Nix files are formatted using <code>nixpkgs-fmt</code>, which is the standard formatter for Nix code.</p>"},{"location":"development/formatting/#configuration","title":"Configuration","text":"<p>The formatter is configured in <code>flake.nix</code>:</p> <pre><code>formatter = forAllSystems (system: nixpkgs.legacyPackages.\"${system}\".nixpkgs-fmt);\n</code></pre>"},{"location":"development/formatting/#usage","title":"Usage","text":"<p>Format all Nix files: <pre><code>nix fmt\n</code></pre></p> <p>Format specific files: <pre><code>nix fmt path/to/file.nix\n</code></pre></p> <p>Check formatting (CI mode): <pre><code>nix fmt --check\n</code></pre></p> <p>Via Justfile: <pre><code>just fmt\n</code></pre></p>"},{"location":"development/formatting/#pre-commit-integration","title":"Pre-commit Integration","text":"<p>The formatter can be run automatically via pre-commit hooks. To set up:</p> <pre><code>just pre-commit-init\n</code></pre>"},{"location":"development/formatting/#yaml-files","title":"YAML Files","text":""},{"location":"development/formatting/#tool-yamllint","title":"Tool: <code>yamllint</code>","text":"<p>YAML files are linted using <code>yamllint</code> with a custom configuration.</p>"},{"location":"development/formatting/#configuration_1","title":"Configuration","text":"<p>Configuration file: <code>.github/lint/.yamllint.yaml</code></p> <p>Key rules: - Line length: disabled (flexibility for long strings) - Indentation: enabled - Comments: minimum 1 space from content - Truthy values: <code>true</code>, <code>false</code>, <code>on</code> allowed</p>"},{"location":"development/formatting/#exclusions","title":"Exclusions","text":"<p>The following are excluded from YAML linting: - <code>.direnv/</code> - <code>.private/</code> - <code>**/*.sops.yaml</code> (encrypted secrets)</p>"},{"location":"development/formatting/#usage_1","title":"Usage","text":"<p>Run manually: <pre><code>yamllint -c .github/lint/.yamllint.yaml &lt;file&gt;\n</code></pre></p> <p>Via pre-commit: Automatically runs on commit for changed YAML files.</p>"},{"location":"development/formatting/#markdown-files","title":"Markdown Files","text":""},{"location":"development/formatting/#standards","title":"Standards","text":"<p>While we don't enforce strict Markdown formatting rules, we follow these guidelines:</p> <ul> <li>Use consistent heading levels</li> <li>Wrap long lines at 80-100 characters when practical</li> <li>Use proper list formatting</li> <li>Include code fences with language identifiers</li> </ul>"},{"location":"development/formatting/#tools","title":"Tools","text":"<p>No automated formatter is currently enforced for Markdown, but you can use:</p> <ul> <li><code>prettier</code> (optional, not configured)</li> <li>Manual formatting following the guidelines above</li> </ul>"},{"location":"development/formatting/#general-guidelines","title":"General Guidelines","text":""},{"location":"development/formatting/#line-endings","title":"Line Endings","text":"<ul> <li>Unix-style line endings (LF) are preferred</li> <li>Pre-commit hooks enforce this automatically</li> </ul>"},{"location":"development/formatting/#trailing-whitespace","title":"Trailing Whitespace","text":"<ul> <li>No trailing whitespace allowed</li> <li>Pre-commit hooks remove it automatically</li> </ul>"},{"location":"development/formatting/#file-encoding","title":"File Encoding","text":"<ul> <li>UTF-8 encoding for all text files</li> <li>Byte order markers (BOM) are not allowed</li> <li>Pre-commit hooks enforce this</li> </ul>"},{"location":"development/formatting/#cicd-integration","title":"CI/CD Integration","text":"<p>Formatting checks are integrated into CI/CD workflows:</p> <ol> <li>Pre-commit hooks run locally before commits</li> <li>GitHub Actions workflows can run formatting checks</li> <li>PR checks validate formatting compliance</li> </ol>"},{"location":"development/formatting/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/formatting/#formatting-conflicts-in-git","title":"Formatting conflicts in Git","text":"<p>If you encounter formatting conflicts:</p> <pre><code># Pull latest changes\ngit pull\n\n# Format all files\nnix fmt\n\n# Resolve conflicts and commit\ngit add .\ngit commit\n</code></pre>"},{"location":"development/formatting/#disabling-formatting-for-specific-lines","title":"Disabling formatting for specific lines","text":"<p>For Nix files, you cannot disable formatting for specific lines with <code>nixpkgs-fmt</code>. If you have a valid reason to format differently, document it in a comment.</p> <p>For YAML files, you can use <code># yamllint disable</code> comments:</p> <pre><code># yamllint disable-line rule:line-length\nvery-long-line-that-would-otherwise-violate-line-length\n</code></pre>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>This guide covers the testing infrastructure and procedures for this NixOS homelab configuration repository.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>Our testing strategy focuses on: - Fast feedback: Quick syntax and structure validation - Configuration validation: Ensuring all hosts and modules evaluate correctly - Import integrity: Verifying all module imports resolve - CI/CD integration: Automated checks on pull requests</p>"},{"location":"development/testing/#test-types","title":"Test Types","text":""},{"location":"development/testing/#1-syntax-validation","title":"1. Syntax Validation","text":"<p>Purpose: Catch basic syntax errors before deeper evaluation.</p> <p>Command: <pre><code>nix-instantiate --parse flake.nix &gt; /dev/null\n</code></pre></p> <p>When to run: Before any other test, catches syntax errors immediately.</p> <p>Duration: &lt; 1 second</p>"},{"location":"development/testing/#2-flake-structure-validation","title":"2. Flake Structure Validation","text":"<p>Purpose: Verify flake metadata and outputs are accessible.</p> <p>Commands: <pre><code># Check metadata\nnix flake metadata --no-write-lock-file\n\n# List outputs\nnix flake show --no-write-lock-file\n</code></pre></p> <p>When to run: After syntax check, validates flake structure.</p> <p>Duration: 2-5 seconds each</p>"},{"location":"development/testing/#3-configuration-evaluation","title":"3. Configuration Evaluation","text":"<p>Purpose: Ensure all NixOS configurations can be evaluated without errors.</p> <p>Commands: <pre><code># Evaluate specific host\nnix eval --impure .#nixosConfigurations.daedalus.config.system.name\n\n# Evaluate all hosts\nfor host in daedalus shodan; do\n  nix eval --impure \".#nixosConfigurations.${host}.config.system.name\"\ndone\n</code></pre></p> <p>When to run: When modifying host configurations or modules.</p> <p>Duration: 10-30 seconds per configuration</p>"},{"location":"development/testing/#4-comprehensive-test-script","title":"4. Comprehensive Test Script","text":"<p>Purpose: Run all validation tests in sequence.</p> <p>Command: <pre><code>./test-flake.sh\n</code></pre></p> <p>What it checks: - Syntax validation - Flake metadata - Flake outputs - Flake check (no build) - Host configuration evaluation - Lib output validation - Application import validation - SOPS secrets encryption state</p> <p>When to run: Before committing significant changes, pre-push.</p> <p>Duration: 30-60 seconds</p> <p>Exit codes: - <code>0</code>: All tests passed - <code>1</code>: One or more tests failed</p>"},{"location":"development/testing/#5-nix-expression-tests","title":"5. Nix Expression Tests","text":"<p>Purpose: Validate flake outputs and structure.</p> <p>Command: <pre><code>nix eval --impure -f test-nix-expressions.nix\n</code></pre></p> <p>What it validates: - All hosts are accessible - Formatter is defined - Lib output exists - DevShells are defined</p> <p>When to run: When modifying flake structure or outputs.</p> <p>Duration: 5-10 seconds</p>"},{"location":"development/testing/#6-import-path-validation","title":"6. Import Path Validation","text":"<p>Purpose: Verify all module imports point to existing files.</p> <p>Manual check: <pre><code># Check service imports\ngrep -E \"applications/[^/]+/[^/]+\" nixos/modules/nixos/services/default.nix | \\\n  while read line; do\n    app=$(echo \"$line\" | sed 's|.*applications/||; s|/default.nix.*||')\n    if [ ! -f \"nixos/modules/applications/${app}/default.nix\" ]; then\n      echo \"ERROR: Missing ${app}\"\n    fi\n  done\n</code></pre></p> <p>When to run: After moving or restructuring application modules.</p> <p>Automated: Included in <code>test-flake.sh</code></p>"},{"location":"development/testing/#7-flake-check-full","title":"7. Flake Check (Full)","text":"<p>Purpose: Complete flake validation including build checks.</p> <p>Command: <pre><code>nix flake check\n</code></pre></p> <p>Warning: This may take longer as it can trigger builds. Use <code>--no-build</code> for faster validation.</p> <p>When to run: Before important deployments, in CI/CD.</p> <p>Duration: Varies (can be minutes)</p>"},{"location":"development/testing/#testing-workflow","title":"Testing Workflow","text":""},{"location":"development/testing/#pre-commit-checklist","title":"Pre-Commit Checklist","text":"<p>Before committing changes:</p> <ol> <li>\u2705 Run <code>nix fmt</code> to format code</li> <li>\u2705 Run <code>just lint</code> to check for linting issues</li> <li>\u2705 Run <code>nix flake check --no-build</code> for fast validation</li> <li>\u2705 Run <code>./test-flake.sh</code> for comprehensive validation</li> </ol>"},{"location":"development/testing/#pre-push-checklist","title":"Pre-Push Checklist","text":"<p>Before pushing to remote:</p> <ol> <li>\u2705 All pre-commit checks pass</li> <li>\u2705 Run full <code>nix flake check</code> (if time permits)</li> <li>\u2705 Verify specific configurations evaluate if modified</li> </ol>"},{"location":"development/testing/#cicd-integration","title":"CI/CD Integration","text":"<p>GitHub Actions automatically runs:</p> <ul> <li><code>nix flake check</code> on pull requests</li> <li>Additional validation in enhanced workflows (see <code>.github/workflows/test-suite.yaml</code>)</li> </ul>"},{"location":"development/testing/#using-justfile-commands","title":"Using Justfile Commands","text":"<p>The repository includes convenient Justfile commands:</p> <pre><code># Format code\njust fmt\n\n# Run linting\njust lint\n\n# Run lint + pre-commit\njust check\n\n# Build configuration for a host\njust build &lt;host&gt;\n\n# Test build (doesn't apply)\njust test &lt;host&gt;\n</code></pre>"},{"location":"development/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/testing/#option-does-not-exist-errors","title":"\"option does not exist\" Errors","text":"<p>Cause: Application module not imported in <code>services/default.nix</code> or <code>containers/default.nix</code></p> <p>Solution: Add the import path with explicit <code>/default.nix</code> suffix: <pre><code>../../applications/media/jellyfin/default.nix\n</code></pre></p>"},{"location":"development/testing/#file-does-not-exist-errors","title":"\"File does not exist\" Errors","text":"<p>Cause: Incorrect relative paths after restructuring</p> <p>Solution: Verify paths use explicit <code>/default.nix</code> suffixes, check relative path from import location</p>"},{"location":"development/testing/#sops-secrets-errors","title":"SOPS Secrets Errors","text":"<p>Cause: Secrets not encrypted or missing keys</p> <p>Solution: Ensure secrets are encrypted with <code>sops --encrypt</code> and correct machine keys are available</p>"},{"location":"development/testing/#circular-dependency-errors","title":"Circular Dependency Errors","text":"<p>Cause: Modules accessing <code>config</code> during option definition</p> <p>Solution: Use <code>mkOption</code> with functions, avoid accessing config in option definitions</p>"},{"location":"development/testing/#advanced-testing","title":"Advanced Testing","text":""},{"location":"development/testing/#vm-testing","title":"VM Testing","text":"<p>For more comprehensive testing, you can build and test in VMs:</p> <pre><code># Build system configuration\nnix build .#nixosConfigurations.&lt;host&gt;.config.system.build.toplevel\n\n# Test in a VM (requires nixos-test)\n# nixos-test &lt;configuration&gt;\n</code></pre>"},{"location":"development/testing/#dry-run-deployments","title":"Dry-Run Deployments","text":"<p>Test deployments without applying:</p> <pre><code># Using deploy-rs\njust dry-deploy &lt;host&gt;\n\n# Using nixos-rebuild\njust dry-activate &lt;host&gt;\n</code></pre>"},{"location":"development/testing/#test-script-output","title":"Test Script Output","text":"<p>The <code>test-flake.sh</code> script provides clear output:</p> <pre><code>\ud83e\uddea Running comprehensive flake validation tests...\n\n1\ufe0f\u20e3  Checking Nix syntax...\n\u2705 Syntax check passed\n\n2\ufe0f\u20e3  Validating flake metadata...\n\u2705 Flake metadata valid\n\n...\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\u2705 All validation tests passed!\n</code></pre> <p>Failures will indicate which step failed for easy debugging.</p>"},{"location":"development/testing/#continuous-improvement","title":"Continuous Improvement","text":"<p>Test coverage should expand as the repository grows:</p> <ul> <li>Consider adding module-specific tests</li> <li>Evaluate adding VM-based integration tests</li> <li>Expand CI/CD test matrix for different scenarios</li> </ul>"},{"location":"installation/install/","title":"Install","text":""},{"location":"installation/install/#getting-isoimage","title":"Getting ISO/Image","text":"<p>How to get nix to a system upfront can be done in different ways depending on how much manual work you plan to do for bootstrapping a system. Below is a few ways I've used, but obviously there will be multiple methods and this isn't a comprehensive list.</p>"},{"location":"installation/install/#default-iso-2-step","title":"Default ISO (2-step)","text":"<p>You can download the minimal ISO from NixOS.org This gets you a basic installer running on tmpfs, which you can then open up for SSH, partition drives, <code>nixos-install</code> them, and then reboot into a basic system, ready to then do a second install for whatever config/flake you are doing.</p>"},{"location":"installation/install/#custom-iso-1-step","title":"Custom ISO (1-step)","text":"<p>An alternative is to build a custom ISO or image with a number of extra tools, ssh key pre-installed, etc. If you image a drive with this you get a bootable nixos install immediately, which I find useful for Raspberry Pi's</p> <p>I have started down this path with the below code:</p> <p>Note</p> <p>The below nix for images is Work In Progress!</p> <p>ISO Image (x86_64) SD Image (aarch64_)</p> <p>From here, you can build them with <code>nix build</code>. I have mine in my flake so I could run <code>nix build .#images.rpi4</code>. Even better, you can also setup a Github Action to build these images for you and put them in a release for you to download.</p>"},{"location":"installation/install/#graphical-install","title":"Graphical install","text":"<p>A graphical install is available but this isn't my cup of tea for a system so closely linked to declarative setup from configuration files. There are plenty of guides to guide through a graphical installer.</p>"},{"location":"installation/install/#alternate-tools-automated","title":"Alternate tools (Automated)","text":"<p>You can look at tools like disko for declarative disk formatting and nixos-anywhere for installing NixOS, well, anywhere using features like <code>kexec</code> to bootstrap nix on any accessiable linux system.</p> <p>This closes the loop of the manual steps you need to setup a NixOS system - I've chosen to avoid these as I don't want additional complexity in my installs and I'm OK with a few manual steps for the rare occasions I setup new devices.</p>"},{"location":"installation/install/#booting-into-raw-system","title":"Booting into raw system","text":""},{"location":"installation/install/#linux-x86_64-systems","title":"Linux x86_64 systems","text":"<p>I'm a fan of Ventoy.net for getting ISO's onto systems, as it allows you to load ISO files onto a USB, and from any system you can boot it up into a ISO selector - this way you can have one USB with many ISO's, including rescue-style ISO's ready to go.</p>"},{"location":"installation/install/#virtual-machines","title":"Virtual Machines","text":"<p>I follow the same approach as x86_64 above for testing Nix in VM's.</p> <p>General settings below for a virtual machine- I stick with larger drives to ensure nix-store &amp; compiling doesn't bite me. 16GB is probably OK too.</p> <p>VM Settings: ISO: nixos-minimal Hard: Drive: 32GB RAM: 2GB EFI: Enable</p> <p>Warning</p> <p>Ensure you have EFI enabled or ensure you align your configuration.nix to your VM's BIOS setup, else you will have issues installing &amp; booting.</p> <p>For VM's, I then expose port 22 to allow SSH from local machine into vm - mapping host port 3022 to vm guest 22.</p>"},{"location":"installation/install/#aarch64-raspi","title":"aarch64 (RasPi)","text":"<p>I cant comment on Mac as im not a Apple guy, so this section is for my Raspberry Pi's. I build my custom image and flash it to a sd-card, then boot it - this way I already have ssh open and I can just ssh into it headless. If you use a minimal ISO you will need to have a monitor/keyboard attached, as by default SSH is not enabled in the default ISO until a root password is set.</p>"},{"location":"installation/install/#other-methods","title":"Other methods","text":"<p>You could also look at PXE/iPXE/network boot across your entire network so devices with no OS 'drop' into a NixOS live installer.</p>"},{"location":"installation/install/#initial-install-from-live-image","title":"Initial install from live image","text":""},{"location":"installation/install/#opening-ssh","title":"Opening SSH","text":"<p>If your live image isn't customised with a root password or ssh key, SSH will be closed until you set one.</p> <pre><code>sudo su\npasswd\n</code></pre> <p><code>sshd</code> is now running, so you can now ssh into the vm remotely for the rest of the setup if you prefer</p>"},{"location":"installation/install/#partitioning-drives","title":"Partitioning drives","text":""},{"location":"installation/install/#standard","title":"Standard","text":"<p>Next step is to partition drives. This will vary per host, so <code>lsblk</code>, viewing disks in <code>/dev/</code>, <code>/dev/disk/by-id</code>, <code>parted</code> and at times your eyeballs will be useful to identify what drive to partition</p> <p>Below is a fairly standard setup with a 512MB boot partition, 8GB swap and the rest ext4. If you have the RAM a swap partition is unlikely to be needed.</p> <pre><code># Partitioning\nparted /dev/sda -- mklabel gpt\nparted /dev/sda -- mkpart root ext4 512MB -8GB\nparted /dev/sda -- mkpart swap linux-swap -8GB 100%\nparted /dev/sda -- mkpart ESP fat32 1MB 512MB\nparted /dev/sda -- set 3 esp on\n</code></pre> <p>And then a little light formatting.</p> <pre><code># Formatting\nmkfs.ext4 -L nixos /dev/sda1\nmkswap -L swap /dev/sda2 # if swap setup\nmkfs.fat -F 32 -n boot /dev/sda3\n</code></pre> <p>We will want to mount these ready for the config generation (which will look at the current mount setup and output config for it)</p> <pre><code># Mounting disks for installation\nmount /dev/disk/by-label/nixos /mnt\nmkdir -p /mnt/boot\nmount /dev/disk/by-label/boot /mnt/boot\nswapon /dev/sda2 # if swap setup\n</code></pre>"},{"location":"installation/install/#impermanence-zfs","title":"Impermanence (ZFS)","text":"<p>TBC</p>"},{"location":"installation/install/#generating-initial-nixos-config","title":"Generating initial nixos config","text":"<p>If this is a fresh machine you've never worked with &amp; dont have a config ready to push to it, you'll need to generate a config to get started</p> <p>The below will generate a config based on the current setup of your machine, and output it to the /mnt folder.</p> <pre><code># Generating default configuration\nnixos-generate-config --root /mnt\n</code></pre> <p>This will output <code>configuration.nix</code> and <code>hardware-config.nix</code>. <code>configuration.nix</code> contains a boilerplate with some basics to create a bootable system, mainly importing hardware-config. <code>hardware-config.nix</code> contains the nix code to setup the kernel on boot with a expected list of modules based on your systems capabilities, as well as the nix to mount the drives as currently setup.</p> <p>As I gitops my files, I then copy the hardware-config to my machine, and then copy across a bootstrap configuration file with some basics added for install.</p> <pre><code>scp -P 3022 nixos/hosts/bootstrap/configuration.nix root@127.0.0.1:/mnt/etc/nixos/configuration.nix\nscp -P 3022 root@127.0.0.1:/mnt/etc/nixos/hardware-configuration.nix nixos/hosts/nixosvm/hardware-configuration.nix\n</code></pre>"},{"location":"installation/install/#installing-nix","title":"Installing nix","text":"<p>From flake:</p> <pre><code>nixos-install --flake github:truxnell/nix-config#daedalus\n</code></pre> <pre><code>nixos-install\nreboot\n\n# after machine has rebooted\nnixos-rebuild switch\n</code></pre> <p>Set the password for the user that was created. Might need to use su?</p> <pre><code>passwd truxnell\n</code></pre> <p>Also grab the ssh keys and re-encrypt sops</p> <pre><code>cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age\n</code></pre> <p>then run task</p> <p>Login as user, copy nix git OR for remote machines/servers just <code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;</code></p> <pre><code>mkdir .local\ncd .local\ngit clone https://github.com/truxnell/nix-config.git\ncd nix-config\n</code></pre> <p>Apply config to bootstrapped device First time around, MUST APPLY  with name of host in ./hosts/ This is because <code>.. --flake .</code> looks for a <code>nixosConfigurations</code> key with the machines hostname The bootstrap machine will be called 'nixos-bootstrap' so the flake by default would resolve <code>nixosConfigurations.nixos-bootstrap</code> Subsequent rebuilds can be called with the default command as after first build the machines hostname will be changed to the desired machine <pre><code>nixos-rebuild switch --flake .#&lt;machinename&gt;\n</code></pre>"},{"location":"maintenance/backups/","title":"Backups","text":"<p>Nightly Backups are facilitated by NixOS's module for restic module and a helper module ive written.</p> <p>This does a nightly ZFS snapshot, in which apps and other mutable data is restic backed up to both a local folder on my NAS and also to Cloudflare R2 . Backing up from a ZFS snapshot ensures that the restic backup is consistent, as backing up files in use (especially a sqlite database) will cause corruption. Here, all restic jobs are backing up as per the 2.05 snapshot, regardless of when they run that night.</p> <p>Another benefit of this approach is that it is service agnostic - containers, nixos services, qemu, whatever all have files in the same place on the filesystem (in the persistant folder) so they can all be backed up in the same fashion.</p> <p>The alternative is to shutdown services during backup (which could be facilitaed with the restic backup pre/post scripts) but ZFS snapshots are a godsend in this area, and im already running them for impermanence.</p> <p>Backing up without snapshots/shutdowns?</p> <p>This is a pattern I see a bit too - if you are backing up files raw without stopping your service beforehand you might want to check to ensure your backups aren't corrupted.</p> <p>The timeline then is:</p> time activity 02.00 ZFS deletes prior snapshot and creates new one, to <code>rpool/safe/persist@restic_nightly_snap</code> 02.05 - 04.05 Restic backs up from new snapshot's hidden read-only mount <code>.zfs</code> with random delays per-service - to local and remote locations"},{"location":"maintenance/backups/#automatic-backups","title":"Automatic Backups","text":"<p>I have added a sops secret for both my local and remote servers in my restic module  /nixos/modules/nixos/services/restic/. This provides the restic password and 'AWS' credentials for the S3-compatible R2 bucket.</p> <p>Backups are created per-service in each services module. This is largely done with a <code>lib</code> helper ive written, which creates both the relevant restic backup local and remote entries in my nixosConfiguration.  nixos/modules/nixos/lib.nix</p> <p>Why not backup the entire persist in one hit?</p> <p>Possibly a hold over from my k8s days, but its incredibly useful to be able to restore per-service, especially if you just want to move an app around or restore one app.  You can always restore multiple repos with a script/taskfile.</p> <p>NixOS will create a service + timer for each job - below shows the output for a prowlarr local/remote backup.</p> <pre><code># Confirming snapshot taken overnight - we can see 2AM\ntruxnell@daedalus ~&gt; systemctl status restic_nightly_snapshot.service\n\u25cb restic_nightly_snapshot.service - Nightly ZFS snapshot for Restic\n     Loaded: loaded (/etc/systemd/system/restic_nightly_snapshot.service; linked; preset: enabled)\n     Active: inactive (dead) since Wed 2024-04-17 02:00:02 AEST; 5h 34min ago\n   Duration: 61ms\nTriggeredBy: \u25cf restic_nightly_snapshot.timer\n    Process: 606080 ExecStart=/nix/store/vd0pr3la91pi0qhmcn7c80rwrn7jkpx9-unit-script-restic_nightly_snapshot-start/bin/restic_nightly_snapshot-start (code=exited, status=0/SUCCESS)\n   Main PID: 606080 (code=exited, status=0/SUCCESS)\n         IP: 0B in, 0B out\n        CPU: 21ms\n# confirming local snapshot occured - we can see 05:05AM\ntruxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n442d4de3  2024-04-17 05:05:04  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n3 snapshots\n\n# confirming remote snapshot occured - we can see 4:52AM\ntruxnell@daedalus ~&gt; sudo restic-prowlarr-remote snapshots\nrepository 30b7eef0 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\ne7d933c4  2024-04-15 22:07:09  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\naa605c6b  2024-04-16 02:39:47  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n68f91a20  2024-04-17 04:52:59  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n3 snapshots\n</code></pre> <p>NixOS (as of 23.05 IIRC) now provides shims to enable easy access to the restic commands with the correct env vars mounted same as the service.</p> <pre><code>truxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n2 snapshots\n</code></pre>"},{"location":"maintenance/backups/#manually-backing-up","title":"Manually backing up","text":"<p>They are a systemd timer/service so you can query or trigger a manual run with <code>systemctl start restic-backups-&lt;service&gt;-&lt;destination&gt;</code> Local and remote work and function exactly the same, querying remote it just a fraction slower to return information.</p> <pre><code>truxnell@daedalus ~ &gt; sudo systemctl start restic-backups-prowlarr-local.service\n&lt; no output &gt;\ntruxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n2 snapshots\ntruxnell@daedalus ~&gt; date\nTue Apr 16 12:43:20 AEST 2024\ntruxnell@daedalus ~&gt;\n</code></pre>"},{"location":"maintenance/backups/#restoring-a-backup","title":"Restoring a backup","text":"<p>Testing a restore (would do --target / for a real restore) Would just have to pause service, run restore, then re-start service.</p> <pre><code>truxnell@daedalus ~ [1]&gt; sudo restic-lidarr-local restore --target /tmp/lidarr/ latest\nrepository a2847581 opened (version 2, compression level auto)\n[0:00] 100.00%  2 / 2 index files loaded\nrestoring &lt;Snapshot b96f4b94 of [/persist/nixos/lidarr] at 2024-04-14 04:19:41.533770692 +1000 AEST by root@daedalus&gt; to /tmp/lidarr/\nSummary: Restored 52581 files/dirs (11.025 GiB) in 1:37\n</code></pre>"},{"location":"maintenance/backups/#failed-backup-notifications","title":"Failed backup notifications","text":"<p>Failed backup notifications are baked-in due to the global Pushover notification on SystemD unit falure. No config nessecary</p> <p>Here I tested it by giving the systemd unit file a incorrect path.</p> A deliberately failed backup to test notifications, hopefully I don't see a real one."},{"location":"maintenance/backups/#disabled-backup-warnings","title":"Disabled backup warnings","text":"<p>Using module warnings, I have also put in warnings into my NixOS modules if I have disabled a warning on a host that isnt a development machine, just in case I do this or mixup flags on hosts. Roll your eyes, I will probably do it. This will pop up when I do a dry run/deployment - but not abort the build.</p> It is eye catching thankfully"},{"location":"maintenance/software_updates/","title":"Software updates","text":"<p>Its crucial to update software regularly - but a homelab isn't a google play store you forget about and let it do its thing. How do you update your software stack regular without breaking things?</p>"},{"location":"maintenance/software_updates/#continuous-integration","title":"Continuous integration","text":"<p>Continuous integration (CI) is running using  Github Actions and Garnix. I have enabled branch protection rules to ensure all my devices successfully build before a PR is allowed to be pushed to main. This ensures I have a level of testing/confidence that an update of a device from the main branch will not break anything.</p> Lovely sea of green passed checks"},{"location":"maintenance/software_updates/#binary-caching","title":"Binary Caching","text":"<p>Binary caching is done for me by Garnix which is an amazing tool. I can then add them as substituter. These run each push to any branch and cache the build results for me. Even better, I can hook into them as above for CI purposes. No code to show here, you add it as an app to your github repo and it 'Just Works '</p> <pre><code># Substitutions\nsubstituters = [ \"https://cache.garnix.io\" ];\n\ntrusted-public-keys = [\n  \"nix-community.cachix.org-1:mB9FSh9qf2dCimDSUo8Zy7bkq5CX+/rkCWyvRCYg3Fs=\"\n];\n</code></pre> Lovely sea of green passed checks"},{"location":"maintenance/software_updates/#flake-updates","title":"Flake updates","text":"<p>Github repo updates are provided by :simple-renovatebot: Renovate by Mend. These are auto-merged on a weekly schedule after passing CI. The settings can be found at  /main/.github/renovate.json5</p> <p>The primary CI is a Garnix build, which Is already building and caching all my systems. Knowing all of the systems have built and cached goes a huge way toward ensuring main is a stable branch.</p>"},{"location":"maintenance/software_updates/#docker-container-updates","title":"Docker container updates","text":"<p>Container updates are provided by :simple-renovatebot: Renovate by Mend. These will either be manually merged after I have checked the upstream projects notes for breaking changes or auto-merged based on settings I have in  /.github/renovate/autoMerge.json5.</p> <p>Semantic Versioning summary</p> <p>Semantic Versioning blurb is a format of MAJOR.MINOR.PATCH: MAJOR version when you make incompatible API changes (e.g. 1.7.8 -&gt; 2.0.0) MINOR version when you add functionality in a backward compatible manner (e.g. 1.7.8 -&gt; 1.8.0) PATCH version when you make backward compatible bug fixes (e.g. 1.7.8 -&gt; 1.7.9)</p> <p>The auto-merge file allows me to define a pattern of which packages I want to auto-merge based on the upgrade type Renovate is suggesting. As many packages adhere to Semantic Versioning, I can determine how I 'feel' about the project, and decide to auto-merge specific tags. So for example, Sonarr has been reliable for me so I am ok merging all digest, patch and minor updates. I will always review a a major update, as it is likely to contain a breaking change.</p> <p>Respect pre-1.0.0 software!</p> <p>Semantic Versioning also specifies that all software before 1.0.0 may have a breaking change AT ANY TIME. Auto update pre 1.0 software at your own risk!</p> <p>The rational here is twofold. One is obvious - The entire point of doing Nix is reproducibility - what is the point of having flakes and SHA tags to provide the ability</p> <p>Also, I dont wan't a trillion PR's in my github repo waiting, but I also will not blindly update everything. There is a balance between updating for security/patching purposes and avoiding breaking changes. I know its popular to use <code>:latest</code> tag and a auto-update service like watchtower - trust me this is a bad idea.</p> I only glanced away from my old homelab for a few months... <p>Automatically updating all versions of containers will break things eventually!</p> <p>This is simply because projects from time to time will release breaking changes - totally different database schemas, overhaul config, replace entire parts of their software stack etc.  If you let your service update totally automatically without checking for these you will wake up to a completely broken service like I did many, many years ago when Seafile did a major upgrade.</p> <p>Container updates are provided by a custom regex that matches my format for defining images in my nix modules.</p> <pre><code>    \"regexManagers\": [\n    {\n      fileMatch: [\"^.*\\\\.nix$\"],\n      matchStrings: [\n        'image *= *\"(?&lt;depName&gt;.*?):(?&lt;currentValue&gt;.*?)(@(?&lt;currentDigest&gt;sha256:[a-f0-9]+))?\";',\n      ],\n      datasourceTemplate: \"docker\",\n    }\n  ],\n</code></pre> <p>And then I can pick and choose what level (if any) I want for container software. The below gives me brackets I can put containers in to enable auto-merge depending on how much I much i trust the container maintainer.</p> <pre><code>  \"packageRules\": [\n    {\n      // auto update up to major\n      \"matchDatasources\": ['docker'],\n      \"automerge\": \"true\",\n      \"automergeType\": \"branch\",\n      \"matchUpdateTypes\": [ 'minor', 'patch', 'digest'],\n      \"matchPackageNames\": [\n        'ghcr.io/home-operations/sonarr',\n        'ghcr.io/home-operations/readarr',\n        'ghcr.io/home-operations/radarr',\n        'ghcr.io/home-operations/lidarr',\n        'ghcr.io/home-operations/prowlarr'\n        'ghcr.io/twin/gatus',\n      ]\n    },\n    // auto update up to minor\n    {\n      \"matchDatasources\": ['docker'],\n      \"automerge\": \"true\",\n      \"automergeType\": \"branch\",\n      \"matchUpdateTypes\": [ 'patch', 'digest'],\n      \"matchPackageNames\": [\n        \"ghcr.io/gethomepage/homepage\",\n      ]\n\n    }\n  ]\n</code></pre> <p>Which results in automated PR's being raised - and possibly automatically merged into main if CI passes.</p> Thankyou RenovateBot!"},{"location":"monitoring/systemd/","title":"SystemD pushover notifications","text":"<p>Keeping with the goal of simple, I put together a <code>curl</code> script that can send me a pushover alert. I originally tied this to individual backups, until I realised how powerful it would be to just have it tied to every SystemD service globally.</p> <p>This way, I would never need to worry or consider what services are being created/destroyed and repeating myself ad nauseam.</p> <p>Why not Prometheus?</p> <p>I ran Prometheus/AlertManager for many years and well it can be easy to get TOO many notifications depending on your alerts, or to have issues with the big complex beast it is itself, or have alerts that trigger/reset/trigger (i.e. HDD temps). This gives me native, simple notifications I can rely on using basic tools - one of my design principles.</p> <p>Immediately I picked up with little effort:</p> <ul> <li>Pod crashloop failed after too many quick restarts</li> <li>Native service failure</li> <li>Backup failures</li> <li>AutoUpdate failure</li> <li>etc</li> </ul> NixOS SystemD built-in notifications for all occasions"},{"location":"monitoring/systemd/#adding-to-all-services","title":"Adding to all services","text":"<p>This is accomplished in /nixos/modules/nixos/system/pushover, with a systemd service <code>notify-pushover@</code>.</p> <p>This can then be called by other services, which I setup with adding into my options:</p> <pre><code>  options.systemd.services = mkOption {\n    type = with types; attrsOf (\n      submodule {\n        config.onFailure = [ \"notify-pushover@%n.service\" ];\n      }\n    );\n</code></pre> <p>This adds into every systemd NixOS generates the \"notify-pushover@%n.service\", where the systemd specifiers are injected with <code>scriptArgs</code>, and the simple bash script can refer to them as <code>$1</code> etc.</p> <pre><code>systemd.services.\"notify-pushover@\" = {\n      enable = true;\n      onFailure = lib.mkForce [ ]; # cant refer to itself on failure (1)\n      description = \"Notify on failed unit %i\";\n      serviceConfig = {\n        Type = \"oneshot\";\n        # User = config.users.users.truxnell.name;\n        EnvironmentFile = config.sops.secrets.\"services/pushover/env\".path; # (2)\n      };\n\n      # Script calls pushover with some deets.\n      # Here im using the systemd specifier %i passed into the script,\n      # which I can reference with bash $1.\n      scriptArgs = \"%i %H\"; # (3)\n      # (4)\n      script = ''\n        ${pkgs.curl}/bin/curl --fail -s -o /dev/null \\\n          --form-string \"token=$PUSHOVER_API_KEY\" \\\n          --form-string \"user=$PUSHOVER_USER_KEY\" \\\n          --form-string \"priority=1\" \\\n          --form-string \"html=1\" \\\n          --form-string \"timestamp=$(date +%s)\" \\\n          --form-string \"url=https://$2:9090/system/services#/$1\" \\\n          --form-string \"url_title=View in Cockpit\" \\\n          --form-string \"title=Unit failure: '$1' on $2\" \\\n          --form-string \"message=&lt;b&gt;$1&lt;/b&gt; has failed on &lt;b&gt;$2&lt;/b&gt;&lt;br&gt;&lt;u&gt;Journal tail:&lt;/u&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;$(journalctl -u $1 -n 10 -o cat)&lt;/i&gt;\" \\\n          https://api.pushover.net/1/messages.json 2&amp;&gt;1\n      '';\n</code></pre> <ol> <li>Force exclude this service from having the default 'onFailure' added</li> <li>Bring in pushover API/User ENV vars for script</li> <li>Pass SystemD specifiers into script</li> <li>Er.. script. Nix pops it into a shell script and refers to it in the unit.</li> </ol> <p>Bug</p> <p>I put in a nice link direct to Cockpit for the specific machine/service in question that doesnt quite work yet... ( #96)</p>"},{"location":"monitoring/systemd/#excluding-from-a-services","title":"Excluding from a services","text":"<p>Now we may not want this on ALL services. Especially the pushover-notify service itself. We can exclude this from a service using Nix <code>nixpkgs.lib.mkForce</code></p> <pre><code># Over-write the default pushover\nsystemd.services.\"service\".onFailure = lib.mkForce [ ] option.\n</code></pre>"},{"location":"monitoring/warnings/","title":"Nix Warnings","text":"<p>I've added warnings and assertations to code using nix to help me avoid misconfigurations. For example, if a module needs a database enabled, it can abort a deployment if it is not enabled. Similary, I have added warnings if I have disabled backups for production machines.</p> <p>But why, when its not being shared with others?</p> <p>Because I guarentee ill somehow stuff it up down the track and accidently disable things I didnt mean to. Roll your eyes, Ill thank myself later.</p> <p>Learnt from: Nix Manual</p>"},{"location":"monitoring/warnings/#warnings","title":"Warnings","text":"<p>Warnings will print a warning message duyring a nix build or deployment, but NOT stop the action. Great for things like reminders on disabled features</p> <p>To add a warning inside a module:</p> <pre><code>    # Warn if backups are disable and machine isn't a dev box\n    config.warnings = [\n      (mkIf (!cfg.local.enable &amp;&amp; config.mySystem.purpose != \"Development\")\n        \"WARNING: Local backups are disabled!\")\n      (mkIf (!cfg.remote.enable &amp;&amp; config.mySystem.purpose != \"Development\")\n        \"WARNING: Remote backups are disabled!\")\n    ];\n</code></pre> Oh THATS what I forgot to re-enable..."},{"location":"monitoring/warnings/#abortassert","title":"Abort/assert","text":"<p>Warnings bigger and meaner brother. Stops a nix build/deploy dead in its tracks. Only useful for when deployment is incompatiable with running - i.e. a dependency not met in options.</p>"},{"location":"monitoring/zed/","title":"Zed","text":"<p>Zed monitoring can also send to pushover!</p> Come on these drives are hardly 12months old"},{"location":"network/dns/","title":"Dns","text":"<p>2 x adguard -&gt; powerdns (authoritive) -&gt; (quad9 || mullvad) note reverse dns (in.arpa) and split brain setup. dnssec</p>"},{"location":"network/dns_dhcp/","title":"DNS &amp; DHCP","text":"<p>TLDR</p> <p>External DNS: Client -&gt; Adguard Home (r-&gt;</p> <p>My DNS has evolved and changed over time, especially with a personal desire to keep my entire internet backbone boring and standard off a trusted vendor. 'Why cant I connect to my Minecraft server' and 'Are you playing with the internet again' are questions I don't want to have to answer in this house.</p> <p>Sadly, while I do love my Unifi Dream Machine Pro, its DNS opportunity is lackluster and I really prefer split-dns so I don't have to access everything with ip:port.</p>"},{"location":"network/dns_dhcp/#general","title":"General","text":"<p> <p>My devices all use the Unifi DHCP server to get addresses, which I much prefer so I maintain all my clients in the single-pane-of-glass the UDMP provides. In the DHCP options, I add the</p>"},{"location":"overview/design/","title":"Design principles","text":"<p>Taking some lead from the Zen of Python:</p> <ul> <li>Minimise dependencies, where required, explicitly define dependencies</li> <li>Use plain Nix &amp; bash to solve problems over additional tooling</li> <li>Stable channel for stable machines. Unstable only where features are important.</li> <li>Modules for a specific service - Profiles for broad configuration of state.</li> <li>Write readable code - descriptive variable names and modules</li> <li>Keep functions/dependencies within the relevant module where possible</li> <li>Errors should never pass silently - use assert etc for misconfigurations</li> <li>Flat is better than nested - use built-in functions like map, filter, and fold to operate on lists or sets</li> </ul>"},{"location":"overview/design/#logic","title":"Logic","text":"<p>Do I have logic in all this mess?  Sort of?</p>"},{"location":"overview/design/#module-use","title":"Module use","text":"<p>I have taken the following approach to using modules for different goals:</p> Daring logic for using profiles and modules <p>Profiles to incrementally build up a hosts shape in layers.  These are 'wide' and touch a broad number of settings to acheive a certain goal:</p> <p>They can be broken down into: - Global profiles - Settings every single machine I'll ever roll will use.  (ex. Timezone, secret config, basic nix settings). - Hardware profile - Settings for a specific hardware platform.  Taps into the nixos-hardware modules and defines my own.  Useful to ensure all my raspi4's have the same hardware setup etc. (ex. grub setup, eeprom updates, kernel modules) - Role profiles - General use of host.  Allows all 'servers' to have the same settings, workstations, development environemtns etc.  (ex. monitoring, log rotation, gui) - Host profiles - Currently left in each hosts file in the hosts folder.  These are machine specific settings that are unique to that host. *(ex. boot disks, services to run on machine, hostname)</p> <p>Modules to define a specific service or setting.  These are a lot tighter in scope and only do what is required to setup one particular thing - however they do still touch a few areas as each module may setup reverse proxy, backups, impermanence, etc - but only exactly what the service needs.</p> <p>This aproach does help massively with DRY.</p>"},{"location":"overview/features/","title":"Features","text":"<p>Some things I'm proud of. Or just happy they exist so I can forget about something until I need to worry.</p> <ul> <li> Nightly BackupsA ZFS snapshot is done at night, with restic then backing up to both locally and cloud.  NixOS wrappers make restoring a single command line entry.ZFS snapshot before backup is important to ensure restic isnt backing up files that are in use, which would cause corruption.</li> <li> Software UpdatesRenovate Bot regulary runs on this Github repo, updating the flake lockfile, containers and other dependencies automatically. Automerge is enabled for updates I expect will be routine, but waits for manual PR approval for updates I suspect may require reading changelog for breaking changes</li> <li> Impermance:Inspried by the Erase your Darlings post, Servers run zfs and rollback to a blank snapshot at night.  This ensures repeatable NixOS deployments and no cruft, and also hardens servers a little.</li> <li> SystemD Notifications:Systemd hook that adds a pushover notification to any systemd unit failure for any unit NixOS is aware of.  No worrying about forgetting to add a notification to every new service or worrying about missing one.</li> </ul>"},{"location":"overview/goals/","title":"Goals","text":"<p>When I set about making this lab I had a number of goals - I wonder how well I will do ?</p> <p>A master list of ideas/goals/etc can be found at  Issue #1</p> <ul> <li> Stability NixOS stable channel for core services unstable for desktop apps/non-mission critical where desired. Containers with SHA256 pinning for server apps</li> <li> KISSKeep it Simple, use boring, reliable, trusted tools - not todays flashy new software repo</li> <li> Easy UpdatesWeekly update schedule, utilizing Renovate for updating lockfile and container images.  Autoupdates enabled off main branch for mission critical. Aim for 'magic rollback' on upgrade failure</li> <li> BackupsNightly restic backups to both cloud and NAS. All databases to have nightly backups. Test backups regulary</li> <li> ReproducabilityFlakes &amp; Git for version pinning, SHA256 tags for containers.</li> <li> MonitoringAutomated monitoring on failure &amp; critical summaries, using basic tools. Use Gatus for both internal and external monitoring</li> <li> Continuous IntegrationCI against main branch to ensure all code compiles OK. Use PR's to add to main and dont skip CI due to impatience. Comprehensive testing infrastructure with automated validation, linting, and formatting checks</li> <li> SecurityDont use containers with S6 overlay/root (i.e. LSIO ). Expose minimal ports at router, Reduce attack surface by keeping it simple, review hardening containers/podman/NixOS</li> <li> Ease of administrationLean into the devil that is SystemD - and have one standard interface to see logs, manipulate services, etc. Run containers as podman services, and webui's for watching/debugging</li> <li> Secrets <sub>ssshh</sub>..Sops-nix for secrets, living in my gitrepo. Avoid cloud services like I used in k8s (i.e. Doppler.io)</li> </ul>"},{"location":"overview/k8s/","title":"K8s","text":"<p>Removed complexity</p> <ul> <li>external secrets -&gt; bog standard sops</li> <li>HA file storage -&gt; standard file system</li> <li>HA database cluster -&gt; nixos standard cluster</li> <li>Database user operator -&gt; nixos standard ensure_users</li> <li>Database permissions operator -&gt; why even??</li> <li>secrets reloader -&gt; sops restart_unit</li> <li>easier managment, all services run through systemd for consistency, cockpit makes viewing logs/pod console etc easy.</li> </ul>"},{"location":"overview/options/","title":"Options","text":"<p>Explain mySystem and myHome</p>"},{"location":"overview/structure/","title":"Repository Structure","text":"<p>Note</p> <p>Oh god writing this now is a horrid idea, I always refactor like 50 times...</p> <p>Here is a bit of a walkthrough of the repository structure so you I can have a vague idea on what is going on. Organizing a monorepo is hard at the best of times. </p> <pre><code>\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 renovate            Renovate modules\n\u2502   \u251c\u2500\u2500 workflows             Github Action workflows (i.e. CI/Site building)\n\u2502   \u2514\u2500\u2500 renovate.json5        Renovate core settings\n\u251c\u2500\u2500 docs                    This mkdocs-material site\n\u2502   nixos                   Nixos Modules\n\u2502   \u2514\u2500\u2500 home                  home-manager nix files\n\u2502       \u251c\u2500\u2500 modules             home-manager modules\n\u2502       \u2514\u2500\u2500 truxnell            home-manager user\n\u2502   \u251c\u2500\u2500 hosts                 hosts for nix - starting point of configs.\n\u2502   \u251c\u2500\u2500 modules               nix modules\n\u2502   \u251c\u2500\u2500 overlays              nixpkgs overlays\n\u2502   \u251c\u2500\u2500 pkgs                  custom nix packages\n\u2502   \u2514\u2500\u2500 profiles              host profiles\n\u251c\u2500\u2500 README.md               Github Repo landing page\n\u251c\u2500\u2500 flake.nix               Core flake\n\u251c\u2500\u2500 flake.lock              Lockfile\n\u251c\u2500\u2500 LICENSE                 Project License\n\u2514\u2500\u2500 mkdocs.yml              mkdocs settings\n</code></pre> <p>Whew that wasnt so hard right... right?</p>"},{"location":"security/containers/","title":"Containers","text":""},{"location":"security/containers/#container-images","title":"Container images","text":"<p>Dont use LSIO!</p>"},{"location":"vm/faq/","title":"Faq","text":""},{"location":"vm/faq/#why-not-recurse-the-module-folder","title":"Why not recurse the module folder","text":"<p>Imports are special in NIX and its important that they are defined at runtime for lazy evaluation - if you do optional/coded imports not everything is available for evaluating.</p>"},{"location":"vm/impermance/","title":"Impermance","text":"<ul> <li>need to save ssh keys on reboot</li> <li>else you end up with sops issues &amp; ssh known_key changes every reboot</li> <li>need to sort out password</li> </ul>"},{"location":"vm/installing-x86_64/","title":"Installing x86 64","text":""},{"location":"vm/installing-x86_64/#installing-a-playground-vm","title":"Installing a playground VM","text":"<p>I've used gnome-boxes from my current Fedora laptop for running playground vm's.</p> <p>Settings: ISO: nixos-minimal Hard drive: 32GB RAM: 2GB EFI: Enable</p> <p>Expose port 22 to allow ssh into vm (host port 3022, guest 22)</p> <pre><code># set temp root passwd\nsudo su\npasswd\n</code></pre> <p><code>sshd</code> is already running, so you can now ssh into the vm remotely for the rest of the setup. <code>ssh root@127.0.0.1 -p 3022</code></p> <pre><code># Partitioning\nparted /dev/sda -- mklabel gpt\nparted /dev/sda -- mkpart root ext4 512MB -8GB\nparted /dev/sda -- mkpart swap linux-swap -8GB 100%\nparted /dev/sda -- mkpart ESP fat32 1MB 512MB\nparted /dev/sda -- set 3 esp on\n\n# Formatting\nmkfs.ext4 -L nixos /dev/sda1\nmkswap -L swap /dev/sda2\nmkfs.fat -F 32 -n boot /dev/sda3\n\n# Mounting disks for installation\nmount /dev/disk/by-label/nixos /mnt\nmkdir -p /mnt/boot\nmount /dev/disk/by-label/boot /mnt/boot\nswapon /dev/sda2\n\n# Generating default configuration\nnixos-generate-config --root /mnt\n</code></pre> <p>From this config copy the bootstrap configuration and fetch the hardware configuration.</p> <pre><code>scp -P 3022 nixos/hosts/bootstrap/configuration.nix root@127.0.0.1:/mnt/etc/nixos/configuration.nix\nscp -P 3022 root@127.0.0.1:/mnt/etc/nixos/hardware-configuration.nix nixos/hosts/nixosvm/hardware-configuration.nix\n</code></pre> <p>Then back to the VM</p> <pre><code>nixos-install\nreboot\nnixos-rebuild switch\n</code></pre> <p>Set the password for the user that was created. Might need to use su?</p> <pre><code>passwd truxnell\n</code></pre> <p>Also grab the ssh keys and re-encrypt sops</p> <pre><code>cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age\n</code></pre> <p>then run task</p> <p>Login as user, copy nix git OR for remote machines/servers just <code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;</code></p> <pre><code>mkdir .local\ncd .local\ngit clone https://github.com/truxnell/nix-config.git\ncd nix-config\n</code></pre> <p>Apply config to bootstrapped device First time around, MUST APPLY  with name of host in ./hosts/ This is because <code>.. --flake .</code> looks for a <code>nixosConfigurations</code> key with the machines hostname The bootstrap machine will be called 'nixos-bootstrap' so the flake by default would resolve <code>nixosConfigurations.nixos-bootstrap</code> Subsequent rebuilds can be called with the default command as after first build the machines hostname will be changed to the desired machine <pre><code>nixos-rebuild switch --flake .#&lt;machinename&gt;\n</code></pre> <p>NOTE: do secrets for sops and shit!!</p>"},{"location":"vm/installing-zfs-impermance/","title":"Installing zfs impermance","text":"<p>https://grahamc.com/blog/erase-your-darlings/</p>"},{"location":"vm/installing-zfs-impermance/#get-hostid","title":"Get hostid","text":"<p>run <code>head -c 8 /etc/machine-id</code> and copy into networking.hostId to ensure ZFS doesnt get borked on reboot</p>"},{"location":"vm/installing-zfs-impermance/#partitioning","title":"Partitioning","text":"<p>parted /dev/sda -- mklabel gpt parted /dev/sda -- mkpart root ext4 512MB -8GB parted /dev/sda -- mkpart ESP fat32 1MB 512MB parted /dev/sda -- set 2 esp on</p>"},{"location":"vm/installing-zfs-impermance/#formatting","title":"Formatting","text":"<p>mkswap -L swap /dev/sdap2 swapon /dev/sdap2 mkfs.fat -F 32 -n boot /dev/sdap3</p>"},{"location":"vm/installing-zfs-impermance/#zfs-on-root-partition","title":"ZFS on root partition","text":"<p>zpool create -O mountpoint=none rpool /dev/sdap1</p> <p>zfs create -p -o mountpoint=legacy rpool/local/root</p>"},{"location":"vm/installing-zfs-impermance/#immediate-blank-snapshot","title":"immediate blank snapshot","text":"<p>zfs snapshot rpool/local/root@blank mount -t zfs rpool/local/root /mnt</p>"},{"location":"vm/installing-zfs-impermance/#boot-partition","title":"Boot partition","text":"<p>mkdir /mnt/boot mount /dev/sdap3 /mnt/boot</p>"},{"location":"vm/installing-zfs-impermance/#mk-nix","title":"mk nix","text":"<p>zfs create -p -o mountpoint=legacy rpool/local/nix mkdir /mnt/nix mount -t zfs rpool/local/nix /mnt/nix</p>"},{"location":"vm/installing-zfs-impermance/#and-a-dataset-for-home-if-needed","title":"And a dataset for /home: if needed","text":"<p>zfs create -p -o mountpoint=legacy rpool/safe/home mkdir /mnt/home mount -t zfs rpool/safe/home /mnt/home</p> <p>zfs create -p -o mountpoint=legacy rpool/safe/persist mkdir /mnt/persist mount -t zfs rpool/safe/persist /mnt/persist</p> <p>Set <code>networking.hostId`` in the nixos config to</code>head -c 8 /etc/machine-id`</p> <pre><code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;\n</code></pre> <p>consider a nixos-enter to import a zpool if required (for NAS) instead of rebooting post-install</p> <p>NOTE: do secrets for sops and shit!!</p>"},{"location":"vm/secrets/","title":"Secrets","text":"<p>Secrets is always a challenge for systems that work like Infrastructure-as-Code (IAC).  I have taken the approach of using sops-nix as I am familiar with the sops project, like it and like the age key system.</p> <p>How im using sops-nix is:</p> <ul> <li>Take a encrypted file that is in a folder/repo</li> <li>Upon <code>nixos-rebuild</code> commands decrypt the file with the hosts ssh key</li> <li>Place the unencrypted file in <code>/run/secrets/</code> folder with specific user/group/permissions</li> <li>Services can then reference this files in a number of ways to ingest the secret.</li> </ul>"},{"location":"vm/secrets/#setup","title":"Setup","text":"<p>There are setup instructions in for a initial setup of sops-nix in the repository.  At a core, you will want to  * Get sops-nix into your flake (Docs at: https://github.com/Mic92/sops-nix) * Create the <code>.sops.yaml</code> file in the root of the git repo (Docs at: https://github.com/Mic92/sops-nix) * Populate keys from hosts (preferably by <code>nix-shell -p ssh-to-age --run 'cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age'</code> on each host) * Encrypt each secret in a <code>secrets.sops.yaml</code> file (`sops -e -i path/to/file/filename.sops.yaml) * Populate secrets in your nix </p> <p>Info</p> <pre><code>I have chosen to let each host have a unique age key, generated by its ssh-key, which is generated unique by nix at install.  This means I have a key per host in my `.sops.yaml` file, and each machine can decrypt the secret with its own key.\nAnother approach is to generate one master key, which Is then pushed to each machine.  I chose not to do this as there is some small security benefit of having a unique key per host.\n</code></pre>"},{"location":"vm/secrets/#adding-new-hosts","title":"Adding new hosts","text":"<p>On new machine, run below to transfer its shiny new ed25519 to age</p> <pre><code>nix-shell -p ssh-to-age --run 'cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age'\n</code></pre> <p>Copy this into <code>./.sops.yaml</code> in base repo, then re-run taskfile <code>task sops:re-encrypt</code> to loop through all sops keys, decrypt then re-encrypt</p>"}]}