{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"readme.md","text":"<p>\ud83d\udc4b Welcome to my NixoOS home and homelab configuration. This monorepo is my personal  nix/nixos setup for all my devices, specifically my homelab.</p> <p>This is the end result of a recovering  k8s addict - who no longer enjoyed the time and effort I personally found it took to run k8s at home.</p>"},{"location":"#why","title":"Why?","text":"<p>Having needed a break from hobby's for some health related reasons, I found coming back to a unpatched cluster a chore, which was left unattented. Then a cheap SSD in my custom VyOS router blew, leading me to just put back in my Unifi Dreammachine router, which broke the custom DNS I was running for my cluster, which caused it issues.</p> <p>During fixing the DNS issue, a basic software upgrade for the custom k8s OS I was running k8s on broke my cluster for the 6th time running, coupled with using a older version of the script tool I used to manage its machine config yaml, which ended up leading to my 6th k8s disaster recovery ).</p> <p>Looking at my boring  Ubuntu ZFS nas which just ran and ran and ran without needing TLC, and remembering the old days with Ubuntu + Docker Compose being hands-off ), I dove into nix, with the idea of getting back to basics of boring proven tools, with the power of nix's declarative system.</p>"},{"location":"#goals","title":"Goals","text":"<p>One of my goals is to bring what I learnt running k8s at home with some of the best homelabbers, into the nix world and see just how much of the practices I learnt I can apply to a nix setup, while focussing on having a solid, reliable, setup that I can leave largely unattended for months without issues cropping up.</p> <p>The goal of this doc is for me to slow down a bit and jot down how and why I am doing what im doing in a module, and cover how I have approached the faucets of homelabbing, so YOU can understand, steal with pride from my code, and hopefully(?) learn a thing or two.</p> <p>To teach me a thing or two, contact me or raise a Issue. PR's may or may not be taken as a personal attack - this is my home setup after all.</p>"},{"location":"motd/","title":"Message of the day","text":"<p>Why not include a nice message of the day for each server I log into?</p> <p>The below gives some insight into what the servers running, status of zpools, usage, etc. While not show below - thankfully - If a zpool error is found the status gives a full <code>zpool status -x</code> debrief which is particulary eye-catching upon login.</p> <p>I've also squeezed in a 'reboot required' flag for when the server had detected its running kernel/init/systemd is a different version to what it booted with - useful to know when long running servers require a reboot to pick up new kernel/etc versions.</p> Message of the day <p>Code TLDR</p> <p>/nixos/modules/nixos/system/motd</p> <p>Write a shell script using nix with a bash motd of your choosing.</p> <pre><code>let\n  motd = pkgs.writeShellScriptBin \"motd\"\n    ''\n      #! /usr/bin/env bash\n      source /etc/os-release\n      service_status=$(systemctl list-units | grep podman-)\n\n      &lt;- SNIP -&gt;\n      printf \"$BOLDService status$ENDCOLOR\\n\"\n    '';\nin\n</code></pre> <p>This gets us a shells script we can then directly call into systemPackages - and after that its just a short hop to make this part of the shell init.</p> <p>Note</p> <p>Replace with your preferred shell!</p> <pre><code>environment.systemPackages = [\n    motd\n];\nprograms.fish.interactiveShellInit =  ''\n    motd\n'';\n</code></pre>"},{"location":"postgres/","title":"Postgres","text":"<p>to restore: dropdb createdb alter owner of db psql &lt; restore.sql</p>"},{"location":"tips/","title":"Tips","text":"<ul> <li>Dont make conditional imports (nix needs to resolve imports upfront)</li> <li>can pass between nixos and home-manager with config.homemanager.users.. and osConfig.&lt;x? <li>when adding home-manager to existing setup, the home-manager service may fail due to trying to over-write existing files in <code>~</code>.  Deleting these should allow the service to start</li> <li>yaml = json, so using nix + builtins.toJSON a lot (and repl to vscode for testing)</li> <p>checking values:</p>"},{"location":"tips/#httpsgithubcomnixosnixpkgsblob90055d5e616bd943795d38808c94dbf0dd35abe8nixosmodulesconfigusers-groupsnixl116","title":"https://github.com/NixOS/nixpkgs/blob/90055d5e616bd943795d38808c94dbf0dd35abe8/nixos/modules/config/users-groups.nix#L116","text":""},{"location":"installation/install/","title":"Install","text":""},{"location":"installation/install/#getting-isoimage","title":"Getting ISO/Image","text":"<p>How to get nix to a system upfront can be done in different ways depending on how much manual work you plan to do for bootstrapping a system. Below is a few ways I've used, but obviously there will be multiple methods and this isn't a comprehensive list.</p>"},{"location":"installation/install/#default-iso-2-step","title":"Default ISO (2-step)","text":"<p>You can download the minimal ISO from NixOS.org This gets you a basic installer running on tmpfs, which you can then open up for SSH, partition drives, <code>nixos-install</code> them, and then reboot into a basic system, ready to then do a second install for whatever config/flake you are doing.</p>"},{"location":"installation/install/#custom-iso-1-step","title":"Custom ISO (1-step)","text":"<p>An alternative is to build a custom ISO or image with a number of extra tools, ssh key pre-installed, etc. If you image a drive with this you get a bootable nixos install immediately, which I find useful for Raspberry Pi's</p> <p>I have started down this path with the below code:</p> <p>Note</p> <p>The below nix for images is Work In Progress!</p> <p>ISO Image (x86_64) SD Image (aarch64_)</p> <p>From here, you can build them with <code>nix build</code>. I have mine in my flake so I could run <code>nix build .#images.rpi4</code>. Even better, you can also setup a Github Action to build these images for you and put them in a release for you to download.</p>"},{"location":"installation/install/#graphical-install","title":"Graphical install","text":"<p>A graphical install is available but this isn't my cup of tea for a system so closely linked to declarative setup from configuration files. There are plenty of guides to guide through a graphical installer.</p>"},{"location":"installation/install/#alternate-tools-automated","title":"Alternate tools (Automated)","text":"<p>You can look at tools like disko for declarative disk formatting and nixos-anywhere for installing NixOS, well, anywhere using features like <code>kexec</code> to bootstrap nix on any accessiable linux system.</p> <p>This closes the loop of the manual steps you need to setup a NixOS system - I've chosen to avoid these as I don't want additional complexity in my installs and I'm OK with a few manual steps for the rare occasions I setup new devices.</p>"},{"location":"installation/install/#booting-into-raw-system","title":"Booting into raw system","text":""},{"location":"installation/install/#linux-x86_64-systems","title":"Linux x86_64 systems","text":"<p>I'm a fan of Ventoy.net for getting ISO's onto systems, as it allows you to load ISO files onto a USB, and from any system you can boot it up into a ISO selector - this way you can have one USB with many ISO's, including rescue-style ISO's ready to go.</p>"},{"location":"installation/install/#virtual-machines","title":"Virtual Machines","text":"<p>I follow the same approach as x86_64 above for testing Nix in VM's.</p> <p>General settings below for a virtual machine- I stick with larger drives to ensure nix-store &amp; compiling doesn't bite me. 16GB is probably OK too.</p> <p>VM Settings: ISO: nixos-minimal Hard: Drive: 32GB RAM: 2GB EFI: Enable</p> <p>Warning</p> <p>Ensure you have EFI enabled or ensure you align your configuration.nix to your VM's BIOS setup, else you will have issues installing &amp; booting.</p> <p>For VM's, I then expose port 22 to allow SSH from local machine into vm - mapping host port 3022 to vm guest 22.</p>"},{"location":"installation/install/#aarch64-raspi","title":"aarch64 (RasPi)","text":"<p>I cant comment on Mac as im not a Apple guy, so this section is for my Raspberry Pi's. I build my custom image and flash it to a sd-card, then boot it - this way I already have ssh open and I can just ssh into it headless. If you use a minimal ISO you will need to have a monitor/keyboard attached, as by default SSH is not enabled in the default ISO until a root password is set.</p>"},{"location":"installation/install/#other-methods","title":"Other methods","text":"<p>You could also look at PXE/iPXE/network boot across your entire network so devices with no OS 'drop' into a NixOS live installer.</p>"},{"location":"installation/install/#initial-install-from-live-image","title":"Initial install from live image","text":""},{"location":"installation/install/#opening-ssh","title":"Opening SSH","text":"<p>If your live image isn't customised with a root password or ssh key, SSH will be closed until you set one.</p> <pre><code>sudo su\npasswd\n</code></pre> <p><code>sshd</code> is now running, so you can now ssh into the vm remotely for the rest of the setup if you prefer</p>"},{"location":"installation/install/#partitioning-drives","title":"Partitioning drives","text":""},{"location":"installation/install/#standard","title":"Standard","text":"<p>Next step is to partition drives. This will vary per host, so <code>lsblk</code>, viewing disks in <code>/dev/</code>, <code>/dev/disk/by-id</code>, <code>parted</code> and at times your eyeballs will be useful to identify what drive to partition</p> <p>Below is a fairly standard setup with a 512MB boot partition, 8GB swap and the rest ext4. If you have the RAM a swap partition is unlikely to be needed.</p> <pre><code># Partitioning\nparted /dev/sda -- mklabel gpt\nparted /dev/sda -- mkpart root ext4 512MB -8GB\nparted /dev/sda -- mkpart swap linux-swap -8GB 100%\nparted /dev/sda -- mkpart ESP fat32 1MB 512MB\nparted /dev/sda -- set 3 esp on\n</code></pre> <p>And then a little light formatting.</p> <pre><code># Formatting\nmkfs.ext4 -L nixos /dev/sda1\nmkswap -L swap /dev/sda2 # if swap setup\nmkfs.fat -F 32 -n boot /dev/sda3\n</code></pre> <p>We will want to mount these ready for the config generation (which will look at the current mount setup and output config for it)</p> <pre><code># Mounting disks for installation\nmount /dev/disk/by-label/nixos /mnt\nmkdir -p /mnt/boot\nmount /dev/disk/by-label/boot /mnt/boot\nswapon /dev/sda2 # if swap setup\n</code></pre>"},{"location":"installation/install/#impermanence-zfs","title":"Impermanence (ZFS)","text":"<p>TBC</p>"},{"location":"installation/install/#generating-initial-nixos-config","title":"Generating initial nixos config","text":"<p>If this is a fresh machine you've never worked with &amp; dont have a config ready to push to it, you'll need to generate a config to get started</p> <p>The below will generate a config based on the current setup of your machine, and output it to the /mnt folder.</p> <pre><code># Generating default configuration\nnixos-generate-config --root /mnt\n</code></pre> <p>This will output <code>configuration.nix</code> and <code>hardware-config.nix</code>. <code>configuration.nix</code> contains a boilerplate with some basics to create a bootable system, mainly importing hardware-config. <code>hardware-config.nix</code> contains the nix code to setup the kernel on boot with a expected list of modules based on your systems capabilities, as well as the nix to mount the drives as currently setup.</p> <p>As I gitops my files, I then copy the hardware-config to my machine, and then copy across a bootstrap configuration file with some basics added for install.</p> <pre><code>scp -P 3022 nixos/hosts/bootstrap/configuration.nix root@127.0.0.1:/mnt/etc/nixos/configuration.nix\nscp -P 3022 root@127.0.0.1:/mnt/etc/nixos/hardware-configuration.nix nixos/hosts/nixosvm/hardware-configuration.nix\n</code></pre>"},{"location":"installation/install/#installing-nix","title":"Installing nix","text":"<p>From flake:</p> <pre><code>nixos-install --flake github:truxnell/nix-config#daedalus\n</code></pre> <pre><code>nixos-install\nreboot\n\n# after machine has rebooted\nnixos-rebuild switch\n</code></pre> <p>Set the password for the user that was created. Might need to use su?</p> <pre><code>passwd truxnell\n</code></pre> <p>Also grab the ssh keys and re-encrypt sops</p> <pre><code>cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age\n</code></pre> <p>then run task</p> <p>Login as user, copy nix git OR for remote machines/servers just <code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;</code></p> <pre><code>mkdir .local\ncd .local\ngit clone https://github.com/truxnell/nix-config.git\ncd nix-config\n</code></pre> <p>Apply config to bootstrapped device First time around, MUST APPLY  with name of host in ./hosts/ This is because <code>.. --flake .</code> looks for a <code>nixosConfigurations</code> key with the machines hostname The bootstrap machine will be called 'nixos-bootstrap' so the flake by default would resolve <code>nixosConfigurations.nixos-bootstrap</code> Subsequent rebuilds can be called with the default command as after first build the machines hostname will be changed to the desired machine <pre><code>nixos-rebuild switch --flake .#&lt;machinename&gt;\n</code></pre>"},{"location":"maintenance/backups/","title":"Backups","text":"<p>Nightly Backups are facilitated by NixOS's module for restic module and a helper module ive written.</p> <p>This does a nightly ZFS snapshot, in which apps and other mutable data is restic backed up to both a local folder on my NAS and also to Cloudflare R2 . Backing up from a ZFS snapshot ensures that the restic backup is consistent, as backing up files in use (especially a sqlite database) will cause corruption. Here, all restic jobs are backing up as per the 2.05 snapshot, regardless of when they run that night.</p> <p>Another benefit of this approach is that it is service agnostic - containers, nixos services, qemu, whatever all have files in the same place on the filesystem (in the persistant folder) so they can all be backed up in the same fashion.</p> <p>The alternative is to shutdown services during backup (which could be facilitaed with the restic backup pre/post scripts) but ZFS snapshots are a godsend in this area, and im already running them for impermanence.</p> <p>Backing up without snapshots/shutdowns?</p> <p>This is a pattern I see a bit too - if you are backing up files raw without stopping your service beforehand you might want to check to ensure your backups aren't corrupted.</p> <p>The timeline then is:</p> time activity 02.00 ZFS deletes prior snapshot and creates new one, to <code>rpool/safe/persist@restic_nightly_snap</code> 02.05 - 04.05 Restic backs up from new snapshot's hidden read-only mount <code>.zfs</code> with random delays per-service - to local and remote locations"},{"location":"maintenance/backups/#automatic-backups","title":"Automatic Backups","text":"<p>I have added a sops secret for both my local and remote servers in my restic module  /nixos/modules/nixos/services/restic/. This provides the restic password and 'AWS' credentials for the S3-compatible R2 bucket.</p> <p>Backups are created per-service in each services module. This is largely done with a <code>lib</code> helper ive written, which creates both the relevant restic backup local and remote entries in my nixosConfiguration.  nixos/modules/nixos/lib.nix</p> <p>Why not backup the entire persist in one hit?</p> <p>Possibly a hold over from my k8s days, but its incredibly useful to be able to restore per-service, especially if you just want to move an app around or restore one app.  You can always restore multiple repos with a script/taskfile.</p> <p>NixOS will create a service + timer for each job - below shows the output for a prowlarr local/remote backup.</p> <pre><code># Confirming snapshot taken overnight - we can see 2AM\ntruxnell@daedalus ~&gt; systemctl status restic_nightly_snapshot.service\n\u25cb restic_nightly_snapshot.service - Nightly ZFS snapshot for Restic\n     Loaded: loaded (/etc/systemd/system/restic_nightly_snapshot.service; linked; preset: enabled)\n     Active: inactive (dead) since Wed 2024-04-17 02:00:02 AEST; 5h 34min ago\n   Duration: 61ms\nTriggeredBy: \u25cf restic_nightly_snapshot.timer\n    Process: 606080 ExecStart=/nix/store/vd0pr3la91pi0qhmcn7c80rwrn7jkpx9-unit-script-restic_nightly_snapshot-start/bin/restic_nightly_snapshot-start (code=exited, status=0/SUCCESS)\n   Main PID: 606080 (code=exited, status=0/SUCCESS)\n         IP: 0B in, 0B out\n        CPU: 21ms\n# confirming local snapshot occured - we can see 05:05AM\ntruxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n442d4de3  2024-04-17 05:05:04  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n3 snapshots\n\n# confirming remote snapshot occured - we can see 4:52AM\ntruxnell@daedalus ~&gt; sudo restic-prowlarr-remote snapshots\nrepository 30b7eef0 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\ne7d933c4  2024-04-15 22:07:09  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\naa605c6b  2024-04-16 02:39:47  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n68f91a20  2024-04-17 04:52:59  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n3 snapshots\n</code></pre> <p>NixOS (as of 23.05 IIRC) now provides shims to enable easy access to the restic commands with the correct env vars mounted same as the service.</p> <pre><code>truxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n2 snapshots\n</code></pre>"},{"location":"maintenance/backups/#manually-backing-up","title":"Manually backing up","text":"<p>They are a systemd timer/service so you can query or trigger a manual run with <code>systemctl start restic-backups-&lt;service&gt;-&lt;destination&gt;</code> Local and remote work and function exactly the same, querying remote it just a fraction slower to return information.</p> <pre><code>truxnell@daedalus ~ &gt; sudo systemctl start restic-backups-prowlarr-local.service\n&lt; no output &gt;\ntruxnell@daedalus ~ [1]&gt; sudo restic-prowlarr-local snapshots\nrepository 9d9bf357 opened (version 2, compression level auto)\nID        Time                 Host        Tags        Paths\n---------------------------------------------------------------------------------------------------------------------\n293dad23  2024-04-15 19:24:37  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n24938fe8  2024-04-16 12:42:50  daedalus                /persist/.zfs/snapshot/restic_nightly_snap/containers/prowlarr\n---------------------------------------------------------------------------------------------------------------------\n2 snapshots\ntruxnell@daedalus ~&gt; date\nTue Apr 16 12:43:20 AEST 2024\ntruxnell@daedalus ~&gt;\n</code></pre>"},{"location":"maintenance/backups/#restoring-a-backup","title":"Restoring a backup","text":"<p>Testing a restore (would do --target / for a real restore) Would just have to pause service, run restore, then re-start service.</p> <pre><code>truxnell@daedalus ~ [1]&gt; sudo restic-lidarr-local restore --target /tmp/lidarr/ latest\nrepository a2847581 opened (version 2, compression level auto)\n[0:00] 100.00%  2 / 2 index files loaded\nrestoring &lt;Snapshot b96f4b94 of [/persist/nixos/lidarr] at 2024-04-14 04:19:41.533770692 +1000 AEST by root@daedalus&gt; to /tmp/lidarr/\nSummary: Restored 52581 files/dirs (11.025 GiB) in 1:37\n</code></pre>"},{"location":"maintenance/backups/#failed-backup-notifications","title":"Failed backup notifications","text":"<p>Failed backup notifications are baked-in due to the global Pushover notification on SystemD unit falure. No config nessecary</p> <p>Here I tested it by giving the systemd unit file a incorrect path.</p> A deliberately failed backup to test notifications, hopefully I don't see a real one."},{"location":"maintenance/backups/#disabled-backup-warnings","title":"Disabled backup warnings","text":"<p>Using module warnings, I have also put in warnings into my NixOS modules if I have disabled a warning on a host that isnt a development machine, just in case I do this or mixup flags on hosts. Roll your eyes, I will probably do it. This will pop up when I do a dry run/deployment - but not abort the build.</p> It is eye catching thankfully"},{"location":"maintenance/software_updates/","title":"Software updates","text":"<p>Its crucial to update software regularly - but a homelab isn't a google play store you forget about and let it do its thing. How do you update your software stack regular without breaking things?</p>"},{"location":"maintenance/software_updates/#continuous-integration","title":"Continuous integration","text":"<p>Continuous integration (CI) is running using  Github Actions and Garnix. I have enabled branch protection rules to ensure all my devices successfully build before a PR is allowed to be pushed to main. This ensures I have a level of testing/confidence that an update of a device from the main branch will not break anything.</p> Lovely sea of green passed checks"},{"location":"maintenance/software_updates/#binary-caching","title":"Binary Caching","text":"<p>Binary caching is done for me by Garnix which is an amazing tool. I can then add them as substituter. These run each push to any branch and cache the build results for me. Even better, I can hook into them as above for CI purposes. No code to show here, you add it as an app to your github repo and it 'Just Works '</p> <pre><code># Substitutions\nsubstituters = [ \"https://cache.garnix.io\" ];\n\ntrusted-public-keys = [\n  \"nix-community.cachix.org-1:mB9FSh9qf2dCimDSUo8Zy7bkq5CX+/rkCWyvRCYg3Fs=\"\n];\n</code></pre> Lovely sea of green passed checks"},{"location":"maintenance/software_updates/#flake-updates","title":"Flake updates","text":"<p>Github repo updates are provided by :simple-renovatebot: Renovate by Mend. These are auto-merged on a weekly schedule after passing CI. The settings can be found at  /main/.github/renovate.json5</p> <p>The primary CI is a Garnix build, which Is already building and caching all my systems. Knowing all of the systems have built and cached goes a huge way toward ensuring main is a stable branch.</p>"},{"location":"maintenance/software_updates/#docker-container-updates","title":"Docker container updates","text":"<p>Container updates are provided by :simple-renovatebot: Renovate by Mend. These will either be manually merged after I have checked the upstream projects notes for breaking changes or auto-merged based on settings I have in  /.github/renovate/autoMerge.json5.</p> <p>Semantic Versioning summary</p> <p>Semantic Versioning blurb is a format of MAJOR.MINOR.PATCH: MAJOR version when you make incompatible API changes (e.g. 1.7.8 -&gt; 2.0.0) MINOR version when you add functionality in a backward compatible manner (e.g. 1.7.8 -&gt; 1.8.0) PATCH version when you make backward compatible bug fixes (e.g. 1.7.8 -&gt; 1.7.9)</p> <p>The auto-merge file allows me to define a pattern of which packages I want to auto-merge based on the upgrade type Renovate is suggesting. As many packages adhere to Semantic Versioning, I can determine how I 'feel' about the project, and decide to auto-merge specific tags. So for example, Sonarr has been reliable for me so I am ok merging all digest, patch and minor updates. I will always review a a major update, as it is likely to contain a breaking change.</p> <p>Respect pre-1.0.0 software!</p> <p>Semantic Versioning also specifies that all software before 1.0.0 may have a breaking change AT ANY TIME. Auto update pre 1.0 software at your own risk!</p> <p>The rational here is twofold. One is obvious - The entire point of doing Nix is reproducibility - what is the point of having flakes and SHA tags to provide the ability</p> <p>Also, I dont wan't a trillion PR's in my github repo waiting, but I also will not blindly update everything. There is a balance between updating for security/patching purposes and avoiding breaking changes. I know its popular to use <code>:latest</code> tag and a auto-update service like watchtower - trust me this is a bad idea.</p> I only glanced away from my old homelab for a few months... <p>Automatically updating all versions of containers will break things eventually!</p> <p>This is simply because projects from time to time will release breaking changes - totally different database schemas, overhaul config, replace entire parts of their software stack etc.  If you let your service update totally automatically without checking for these you will wake up to a completely broken service like I did many, many years ago when Seafile did a major upgrade.</p> <p>Container updates are provided by a custom regex that matches my format for defining images in my nix modules.</p> <pre><code>    \"regexManagers\": [\n    {\n      fileMatch: [\"^.*\\\\.nix$\"],\n      matchStrings: [\n        'image *= *\"(?&lt;depName&gt;.*?):(?&lt;currentValue&gt;.*?)(@(?&lt;currentDigest&gt;sha256:[a-f0-9]+))?\";',\n      ],\n      datasourceTemplate: \"docker\",\n    }\n  ],\n</code></pre> <p>And then I can pick and choose what level (if any) I want for container software. The below gives me brackets I can put containers in to enable auto-merge depending on how much I much i trust the container maintainer.</p> <pre><code>  \"packageRules\": [\n    {\n      // auto update up to major\n      \"matchDatasources\": ['docker'],\n      \"automerge\": \"true\",\n      \"automergeType\": \"branch\",\n      \"matchUpdateTypes\": [ 'minor', 'patch', 'digest'],\n      \"matchPackageNames\": [\n        'ghcr.io/home-operations/sonarr',\n        'ghcr.io/home-operations/readarr',\n        'ghcr.io/home-operations/radarr',\n        'ghcr.io/home-operations/lidarr',\n        'ghcr.io/home-operations/prowlarr'\n        'ghcr.io/twin/gatus',\n      ]\n    },\n    // auto update up to minor\n    {\n      \"matchDatasources\": ['docker'],\n      \"automerge\": \"true\",\n      \"automergeType\": \"branch\",\n      \"matchUpdateTypes\": [ 'patch', 'digest'],\n      \"matchPackageNames\": [\n        \"ghcr.io/gethomepage/homepage\",\n      ]\n\n    }\n  ]\n</code></pre> <p>Which results in automated PR's being raised - and possibly automatically merged into main if CI passes.</p> Thankyou RenovateBot!"},{"location":"monitoring/systemd/","title":"SystemD pushover notifications","text":"<p>Keeping with the goal of simple, I put together a <code>curl</code> script that can send me a pushover alert. I originally tied this to individual backups, until I realised how powerful it would be to just have it tied to every SystemD service globally.</p> <p>This way, I would never need to worry or consider what services are being created/destroyed and repeating myself ad nauseam.</p> <p>Why not Prometheus?</p> <p>I ran Prometheus/AlertManager for many years and well it can be easy to get TOO many notifications depending on your alerts, or to have issues with the big complex beast it is itself, or have alerts that trigger/reset/trigger (i.e. HDD temps). This gives me native, simple notifications I can rely on using basic tools - one of my design principles.</p> <p>Immediately I picked up with little effort:</p> <ul> <li>Pod crashloop failed after too many quick restarts</li> <li>Native service failure</li> <li>Backup failures</li> <li>AutoUpdate failure</li> <li>etc</li> </ul> NixOS SystemD built-in notifications for all occasions"},{"location":"monitoring/systemd/#adding-to-all-services","title":"Adding to all services","text":"<p>This is accomplished in /nixos/modules/nixos/system/pushover, with a systemd service <code>notify-pushover@</code>.</p> <p>This can then be called by other services, which I setup with adding into my options:</p> <pre><code>  options.systemd.services = mkOption {\n    type = with types; attrsOf (\n      submodule {\n        config.onFailure = [ \"notify-pushover@%n.service\" ];\n      }\n    );\n</code></pre> <p>This adds into every systemd NixOS generates the \"notify-pushover@%n.service\", where the systemd specifiers are injected with <code>scriptArgs</code>, and the simple bash script can refer to them as <code>$1</code> etc.</p> <pre><code>systemd.services.\"notify-pushover@\" = {\n      enable = true;\n      onFailure = lib.mkForce [ ]; # cant refer to itself on failure (1)\n      description = \"Notify on failed unit %i\";\n      serviceConfig = {\n        Type = \"oneshot\";\n        # User = config.users.users.truxnell.name;\n        EnvironmentFile = config.sops.secrets.\"services/pushover/env\".path; # (2)\n      };\n\n      # Script calls pushover with some deets.\n      # Here im using the systemd specifier %i passed into the script,\n      # which I can reference with bash $1.\n      scriptArgs = \"%i %H\"; # (3)\n      # (4)\n      script = ''\n        ${pkgs.curl}/bin/curl --fail -s -o /dev/null \\\n          --form-string \"token=$PUSHOVER_API_KEY\" \\\n          --form-string \"user=$PUSHOVER_USER_KEY\" \\\n          --form-string \"priority=1\" \\\n          --form-string \"html=1\" \\\n          --form-string \"timestamp=$(date +%s)\" \\\n          --form-string \"url=https://$2:9090/system/services#/$1\" \\\n          --form-string \"url_title=View in Cockpit\" \\\n          --form-string \"title=Unit failure: '$1' on $2\" \\\n          --form-string \"message=&lt;b&gt;$1&lt;/b&gt; has failed on &lt;b&gt;$2&lt;/b&gt;&lt;br&gt;&lt;u&gt;Journal tail:&lt;/u&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;$(journalctl -u $1 -n 10 -o cat)&lt;/i&gt;\" \\\n          https://api.pushover.net/1/messages.json 2&amp;&gt;1\n      '';\n</code></pre> <ol> <li>Force exclude this service from having the default 'onFailure' added</li> <li>Bring in pushover API/User ENV vars for script</li> <li>Pass SystemD specifiers into script</li> <li>Er.. script. Nix pops it into a shell script and refers to it in the unit.</li> </ol> <p>Bug</p> <p>I put in a nice link direct to Cockpit for the specific machine/service in question that doesnt quite work yet... ( #96)</p>"},{"location":"monitoring/systemd/#excluding-from-a-services","title":"Excluding from a services","text":"<p>Now we may not want this on ALL services. Especially the pushover-notify service itself. We can exclude this from a service using Nix <code>nixpkgs.lib.mkForce</code></p> <pre><code># Over-write the default pushover\nsystemd.services.\"service\".onFailure = lib.mkForce [ ] option.\n</code></pre>"},{"location":"monitoring/warnings/","title":"Nix Warnings","text":"<p>I've added warnings and assertations to code using nix to help me avoid misconfigurations. For example, if a module needs a database enabled, it can abort a deployment if it is not enabled. Similary, I have added warnings if I have disabled backups for production machines.</p> <p>But why, when its not being shared with others?</p> <p>Because I guarentee ill somehow stuff it up down the track and accidently disable things I didnt mean to. Roll your eyes, Ill thank myself later.</p> <p>Learnt from: Nix Manual</p>"},{"location":"monitoring/warnings/#warnings","title":"Warnings","text":"<p>Warnings will print a warning message duyring a nix build or deployment, but NOT stop the action. Great for things like reminders on disabled features</p> <p>To add a warning inside a module:</p> <pre><code>    # Warn if backups are disable and machine isn't a dev box\n    config.warnings = [\n      (mkIf (!cfg.local.enable &amp;&amp; config.mySystem.purpose != \"Development\")\n        \"WARNING: Local backups are disabled!\")\n      (mkIf (!cfg.remote.enable &amp;&amp; config.mySystem.purpose != \"Development\")\n        \"WARNING: Remote backups are disabled!\")\n    ];\n</code></pre> Oh THATS what I forgot to re-enable..."},{"location":"monitoring/warnings/#abortassert","title":"Abort/assert","text":"<p>Warnings bigger and meaner brother. Stops a nix build/deploy dead in its tracks. Only useful for when deployment is incompatiable with running - i.e. a dependency not met in options.</p>"},{"location":"monitoring/zed/","title":"Zed","text":"<p>Zed monitoring can also send to pushover!</p> Come on these drives are hardly 12months old"},{"location":"network/dns/","title":"Dns","text":"<p>2 x adguard -&gt; powerdns (authoritive) -&gt; (quad9 || mullvad) note reverse dns (in.arpa) and split brain setup. dnssec</p>"},{"location":"network/dns_dhcp/","title":"DNS &amp; DHCP","text":"<p>TLDR</p> <p>External DNS: Client -&gt; Adguard Home (r-&gt;</p> <p>My DNS has evolved and changed over time, especially with a personal desire to keep my entire internet backbone boring and standard off a trusted vendor. 'Why cant I connect to my Minecraft server' and 'Are you playing with the internet again' are questions I don't want to have to answer in this house.</p> <p>Sadly, while I do love my Unifi Dream Machine Pro, its DNS opportunity is lackluster and I really prefer split-dns so I don't have to access everything with ip:port.</p>"},{"location":"network/dns_dhcp/#general","title":"General","text":"<p> <p>My devices all use the Unifi DHCP server to get addresses, which I much prefer so I maintain all my clients in the single-pane-of-glass the UDMP provides. In the DHCP options, I add the</p>"},{"location":"overview/design/","title":"Design principles","text":"<p>Taking some lead from the Zen of Python:</p> <ul> <li>Minimise dependencies, where required, explicitly define dependencies</li> <li>Use plain Nix &amp; bash to solve problems over additional tooling</li> <li>Stable channel for stable machines. Unstable only where features are important.</li> <li>Modules for a specific service - Profiles for broad configuration of state.</li> <li>Write readable code - descriptive variable names and modules</li> <li>Keep functions/dependencies within the relevant module where possible</li> <li>Errors should never pass silently - use assert etc for misconfigurations</li> <li>Flat is better than nested - use built-in functions like map, filter, and fold to operate on lists or sets</li> </ul>"},{"location":"overview/design/#logic","title":"Logic","text":"<p>Do I have logic in all this mess?  Sort of?</p>"},{"location":"overview/design/#module-use","title":"Module use","text":"<p>I have taken the following approach to using modules for different goals:</p> Daring logic for using profiles and modules <p>Profiles to incrementally build up a hosts shape in layers.  These are 'wide' and touch a broad number of settings to acheive a certain goal:</p> <p>They can be broken down into: - Global profiles - Settings every single machine I'll ever roll will use.  (ex. Timezone, secret config, basic nix settings). - Hardware profile - Settings for a specific hardware platform.  Taps into the nixos-hardware modules and defines my own.  Useful to ensure all my raspi4's have the same hardware setup etc. (ex. grub setup, eeprom updates, kernel modules) - Role profiles - General use of host.  Allows all 'servers' to have the same settings, workstations, development environemtns etc.  (ex. monitoring, log rotation, gui) - Host profiles - Currently left in each hosts file in the hosts folder.  These are machine specific settings that are unique to that host. *(ex. boot disks, services to run on machine, hostname)</p> <p>Modules to define a specific service or setting.  These are a lot tighter in scope and only do what is required to setup one particular thing - however they do still touch a few areas as each module may setup reverse proxy, backups, impermanence, etc - but only exactly what the service needs.</p> <p>This aproach does help massively with DRY.</p>"},{"location":"overview/features/","title":"Features","text":"<p>Some things I'm proud of. Or just happy they exist so I can forget about something until I need to worry.</p> <ul> <li> Nightly BackupsA ZFS snapshot is done at night, with restic then backing up to both locally and cloud.  NixOS wrappers make restoring a single command line entry.ZFS snapshot before backup is important to ensure restic isnt backing up files that are in use, which would cause corruption.</li> <li> Software UpdatesRenovate Bot regulary runs on this Github repo, updating the flake lockfile, containers and other dependencies automatically. Automerge is enabled for updates I expect will be routine, but waits for manual PR approval for updates I suspect may require reading changelog for breaking changes</li> <li> Impermance:Inspried by the Erase your Darlings post, Servers run zfs and rollback to a blank snapshot at night.  This ensures repeatable NixOS deployments and no cruft, and also hardens servers a little.</li> <li> SystemD Notifications:Systemd hook that adds a pushover notification to any systemd unit failure for any unit NixOS is aware of.  No worrying about forgetting to add a notification to every new service or worrying about missing one.</li> </ul>"},{"location":"overview/goals/","title":"Goals","text":"<p>When I set about making this lab I had a number of goals - I wonder how well I will do ?</p> <p>A master list of ideas/goals/etc can be found at  Issue #1</p> <ul> <li> Stability NixOS stable channel for core services unstable for desktop apps/non-mission critical where desired. Containers with SHA256 pinning for server apps</li> <li> KISSKeep it Simple, use boring, reliable, trusted tools - not todays flashy new software repo</li> <li> Easy UpdatesWeekly update schedule, utilizing Renovate for updating lockfile and container images.  Autoupdates enabled off main branch for mission critical. Aim for 'magic rollback' on upgrade failure</li> <li> BackupsNightly restic backups to both cloud and NAS. All databases to have nightly backups. Test backups regulary</li> <li> ReproducabilityFlakes &amp; Git for version pinning, SHA256 tags for containers.</li> <li> MonitoringAutomated monitoring on failure &amp; critical summaries, using basic tools. Use Gatus for both internal and external monitoring</li> <li> Continuous IntegrationCI against main branch to ensure all code compiles OK. Use PR's to add to main and dont skip CI due to impatience</li> <li> SecurityDont use containers with S6 overlay/root (i.e. LSIO ). Expose minimal ports at router, Reduce attack surface by keeping it simple, review hardening containers/podman/NixOS</li> <li> Ease of administrationLean into the devil that is SystemD - and have one standard interface to see logs, manipulate services, etc. Run containers as podman services, and webui's for watching/debugging</li> <li> Secrets <sub>ssshh</sub>..Sops-nix for secrets, living in my gitrepo. Avoid cloud services like I used in k8s (i.e. Doppler.io)</li> </ul>"},{"location":"overview/k8s/","title":"K8s","text":"<p>Removed complexity</p> <ul> <li>external secrets -&gt; bog standard sops</li> <li>HA file storage -&gt; standard file system</li> <li>HA database cluster -&gt; nixos standard cluster</li> <li>Database user operator -&gt; nixos standard ensure_users</li> <li>Database permissions operator -&gt; why even??</li> <li>secrets reloader -&gt; sops restart_unit</li> <li>easier managment, all services run through systemd for consistency, cockpit makes viewing logs/pod console etc easy.</li> </ul>"},{"location":"overview/options/","title":"Options","text":"<p>Explain mySystem and myHome</p>"},{"location":"overview/structure/","title":"Repository Structure","text":"<p>Note</p> <p>Oh god writing this now is a horrid idea, I always refactor like 50 times...</p> <p>Here is a bit of a walkthrough of the repository structure so you I can have a vague idea on what is going on. Organizing a monorepo is hard at the best of times. </p> <pre><code>\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 renovate            Renovate modules\n\u2502   \u251c\u2500\u2500 workflows             Github Action workflows (i.e. CI/Site building)\n\u2502   \u2514\u2500\u2500 renovate.json5        Renovate core settings\n\u251c\u2500\u2500 .taskfiles              go-task file modules\n\u251c\u2500\u2500 docs                    This mkdocs-material site\n\u2502   nixos                   Nixos Modules\n\u2502   \u2514\u2500\u2500 home                  home-manager nix files\n\u2502       \u251c\u2500\u2500 modules             home-manager modules\n\u2502       \u2514\u2500\u2500 truxnell            home-manager user\n\u2502   \u251c\u2500\u2500 hosts                 hosts for nix - starting point of configs.\n\u2502   \u251c\u2500\u2500 modules               nix modules\n\u2502   \u251c\u2500\u2500 overlays              nixpkgs overlays\n\u2502   \u251c\u2500\u2500 pkgs                  custom nix packages\n\u2502   \u2514\u2500\u2500 profiles              host profiles\n\u251c\u2500\u2500 README.md               Github Repo landing page\n\u251c\u2500\u2500 flake.nix               Core flake\n\u251c\u2500\u2500 flake.lock              Lockfile\n\u251c\u2500\u2500 LICENSE                 Project License\n\u251c\u2500\u2500 mkdocs.yml              mkdocs settings\n\u2514\u2500\u2500 Taskfile.yaml           go-task core file\n</code></pre> <p>Whew that wasnt so hard right... right?</p>"},{"location":"security/containers/","title":"Containers","text":""},{"location":"security/containers/#container-images","title":"Container images","text":"<p>Dont use LSIO!</p>"},{"location":"vm/faq/","title":"Faq","text":""},{"location":"vm/faq/#why-not-recurse-the-module-folder","title":"Why not recurse the module folder","text":"<p>Imports are special in NIX and its important that they are defined at runtime for lazy evaluation - if you do optional/coded imports not everything is available for evaluating.</p>"},{"location":"vm/impermance/","title":"Impermance","text":"<ul> <li>need to save ssh keys on reboot</li> <li>else you end up with sops issues &amp; ssh known_key changes every reboot</li> <li>need to sort out password</li> </ul>"},{"location":"vm/installing-x86_64/","title":"Installing x86 64","text":""},{"location":"vm/installing-x86_64/#installing-a-playground-vm","title":"Installing a playground VM","text":"<p>I've used gnome-boxes from my current Fedora laptop for running playground vm's.</p> <p>Settings: ISO: nixos-minimal Hard drive: 32GB RAM: 2GB EFI: Enable</p> <p>Expose port 22 to allow ssh into vm (host port 3022, guest 22)</p> <pre><code># set temp root passwd\nsudo su\npasswd\n</code></pre> <p><code>sshd</code> is already running, so you can now ssh into the vm remotely for the rest of the setup. <code>ssh root@127.0.0.1 -p 3022</code></p> <pre><code># Partitioning\nparted /dev/sda -- mklabel gpt\nparted /dev/sda -- mkpart root ext4 512MB -8GB\nparted /dev/sda -- mkpart swap linux-swap -8GB 100%\nparted /dev/sda -- mkpart ESP fat32 1MB 512MB\nparted /dev/sda -- set 3 esp on\n\n# Formatting\nmkfs.ext4 -L nixos /dev/sda1\nmkswap -L swap /dev/sda2\nmkfs.fat -F 32 -n boot /dev/sda3\n\n# Mounting disks for installation\nmount /dev/disk/by-label/nixos /mnt\nmkdir -p /mnt/boot\nmount /dev/disk/by-label/boot /mnt/boot\nswapon /dev/sda2\n\n# Generating default configuration\nnixos-generate-config --root /mnt\n</code></pre> <p>From this config copy the bootstrap configuration and fetch the hardware configuration.</p> <pre><code>scp -P 3022 nixos/hosts/bootstrap/configuration.nix root@127.0.0.1:/mnt/etc/nixos/configuration.nix\nscp -P 3022 root@127.0.0.1:/mnt/etc/nixos/hardware-configuration.nix nixos/hosts/nixosvm/hardware-configuration.nix\n</code></pre> <p>Then back to the VM</p> <pre><code>nixos-install\nreboot\nnixos-rebuild switch\n</code></pre> <p>Set the password for the user that was created. Might need to use su?</p> <pre><code>passwd truxnell\n</code></pre> <p>Also grab the ssh keys and re-encrypt sops</p> <pre><code>cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age\n</code></pre> <p>then run task</p> <p>Login as user, copy nix git OR for remote machines/servers just <code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;</code></p> <pre><code>mkdir .local\ncd .local\ngit clone https://github.com/truxnell/nix-config.git\ncd nix-config\n</code></pre> <p>Apply config to bootstrapped device First time around, MUST APPLY  with name of host in ./hosts/ This is because <code>.. --flake .</code> looks for a <code>nixosConfigurations</code> key with the machines hostname The bootstrap machine will be called 'nixos-bootstrap' so the flake by default would resolve <code>nixosConfigurations.nixos-bootstrap</code> Subsequent rebuilds can be called with the default command as after first build the machines hostname will be changed to the desired machine <pre><code>nixos-rebuild switch --flake .#&lt;machinename&gt;\n</code></pre> <p>NOTE: do secrets for sops and shit!!</p>"},{"location":"vm/installing-zfs-impermance/","title":"Installing zfs impermance","text":"<p>https://grahamc.com/blog/erase-your-darlings/</p>"},{"location":"vm/installing-zfs-impermance/#get-hostid","title":"Get hostid","text":"<p>run <code>head -c 8 /etc/machine-id</code> and copy into networking.hostId to ensure ZFS doesnt get borked on reboot</p>"},{"location":"vm/installing-zfs-impermance/#partitioning","title":"Partitioning","text":"<p>parted /dev/sda -- mklabel gpt parted /dev/sda -- mkpart root ext4 512MB -8GB parted /dev/sda -- mkpart ESP fat32 1MB 512MB parted /dev/sda -- set 2 esp on</p>"},{"location":"vm/installing-zfs-impermance/#formatting","title":"Formatting","text":"<p>mkswap -L swap /dev/sdap2 swapon /dev/sdap2 mkfs.fat -F 32 -n boot /dev/sdap3</p>"},{"location":"vm/installing-zfs-impermance/#zfs-on-root-partition","title":"ZFS on root partition","text":"<p>zpool create -O mountpoint=none rpool /dev/sdap1</p> <p>zfs create -p -o mountpoint=legacy rpool/local/root</p>"},{"location":"vm/installing-zfs-impermance/#immediate-blank-snapshot","title":"immediate blank snapshot","text":"<p>zfs snapshot rpool/local/root@blank mount -t zfs rpool/local/root /mnt</p>"},{"location":"vm/installing-zfs-impermance/#boot-partition","title":"Boot partition","text":"<p>mkdir /mnt/boot mount /dev/sdap3 /mnt/boot</p>"},{"location":"vm/installing-zfs-impermance/#mk-nix","title":"mk nix","text":"<p>zfs create -p -o mountpoint=legacy rpool/local/nix mkdir /mnt/nix mount -t zfs rpool/local/nix /mnt/nix</p>"},{"location":"vm/installing-zfs-impermance/#and-a-dataset-for-home-if-needed","title":"And a dataset for /home: if needed","text":"<p>zfs create -p -o mountpoint=legacy rpool/safe/home mkdir /mnt/home mount -t zfs rpool/safe/home /mnt/home</p> <p>zfs create -p -o mountpoint=legacy rpool/safe/persist mkdir /mnt/persist mount -t zfs rpool/safe/persist /mnt/persist</p> <p>Set <code>networking.hostId`` in the nixos config to</code>head -c 8 /etc/machine-id`</p> <pre><code>nixos-install --impure --flake github:truxnell/nix-config#&lt;MACHINE_ID&gt;\n</code></pre> <p>consider a nixos-enter to import a zpool if required (for NAS) instead of rebooting post-install</p> <p>NOTE: do secrets for sops and shit!!</p>"},{"location":"vm/secrets/","title":"Secrets","text":"<p>Secrets is always a challenge for systems that work like Infrastructure-as-Code (IAC).  I have taken the approach of using sops-nix as I am familiar with the sops project, like it and like the age key system.</p> <p>How im using sops-nix is:</p> <ul> <li>Take a encrypted file that is in a folder/repo</li> <li>Upon <code>nixos-rebuild</code> commands decrypt the file with the hosts ssh key</li> <li>Place the unencrypted file in <code>/run/secrets/</code> folder with specific user/group/permissions</li> <li>Services can then reference this files in a number of ways to ingest the secret.</li> </ul>"},{"location":"vm/secrets/#setup","title":"Setup","text":"<p>There are setup instructions in for a initial setup of sops-nix in the repository.  At a core, you will want to  * Get sops-nix into your flake (Docs at: https://github.com/Mic92/sops-nix) * Create the <code>.sops.yaml</code> file in the root of the git repo (Docs at: https://github.com/Mic92/sops-nix) * Populate keys from hosts (preferably by <code>nix-shell -p ssh-to-age --run 'cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age'</code> on each host) * Encrypt each secret in a <code>secrets.sops.yaml</code> file (`sops -e -i path/to/file/filename.sops.yaml) * Populate secrets in your nix </p> <p>Info</p> <pre><code>I have chosen to let each host have a unique age key, generated by its ssh-key, which is generated unique by nix at install.  This means I have a key per host in my `.sops.yaml` file, and each machine can decrypt the secret with its own key.\nAnother approach is to generate one master key, which Is then pushed to each machine.  I chose not to do this as there is some small security benefit of having a unique key per host.\n</code></pre>"},{"location":"vm/secrets/#adding-new-hosts","title":"Adding new hosts","text":"<p>On new machine, run below to transfer its shiny new ed25519 to age</p> <pre><code>nix-shell -p ssh-to-age --run 'cat /etc/ssh/ssh_host_ed25519_key.pub | ssh-to-age'\n</code></pre> <p>Copy this into <code>./.sops.yaml</code> in base repo, then re-run taskfile <code>task sops:re-encrypt</code> to loop through all sops keys, decrypt then re-encrypt</p>"}]}